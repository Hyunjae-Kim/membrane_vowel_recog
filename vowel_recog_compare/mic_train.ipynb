{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'mic'\n",
    "model_num = 2\n",
    "time_len =  20  ## ms\n",
    "point_len = int(time_len*10)\n",
    "div_num = int(100*(20/time_len))\n",
    "vow_num = int(div_num/5)\n",
    "\n",
    "fc_len = int(point_len/32)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, loss):\n",
    "        super(Model, self).__init__()\n",
    "        prob = 0.8\n",
    "        input_c = 10\n",
    "        channel = 32\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_c, channel, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(channel, channel*2, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*2))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(channel*2, channel*2, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*2))\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(channel*2, channel*4, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*4))\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(channel*4, channel*4, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*4))\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(channel*4*fc_len, 1024),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(1024))\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 5),\n",
    "            nn.Softmax(dim=1))\n",
    "        self.loss = loss\n",
    "        \n",
    "    def forward(self, data, target):\n",
    "        x = self.conv1(data)\n",
    "#         print('1',x.size())\n",
    "        x = self.conv2(x)\n",
    "#         print('2',x.size())\n",
    "        x = self.conv3(x)\n",
    "#         print('3',x.size())\n",
    "        x = self.conv4(x)\n",
    "#         print('4',x.size())\n",
    "        x = self.conv5(x)\n",
    "#         print('5',x.size())\n",
    "        x = x.view(x.size()[0],-1)\n",
    "        x = self.fc1(x)\n",
    "#         print('8',x.size())\n",
    "        h = self.fc2(x)\n",
    "#         print('9',h.size())\n",
    "        \n",
    "        l = self.loss(h, target)\n",
    "        return l, h, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainX, trainY, batch, device, optimizer, train_loss, train_acc):\n",
    "    model.train()\n",
    "    dloss = 0\n",
    "    dacc = 0\n",
    "    \n",
    "    rand = torch.randperm(trainX.size()[0])\n",
    "    trainX = trainX[rand]\n",
    "    trainY = trainY[rand]\n",
    "    \n",
    "    for i in range(batch[0]):\n",
    "        optimizer.zero_grad()\n",
    "        loss, output, target = model(trainX[i*batch[1]:(i+1)*batch[1]], trainY[i*batch[1]:(i+1)*batch[1]])\n",
    "        loss = loss.sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, output = torch.max(output, 1)\n",
    "        _, target = torch.max(target, 1)\n",
    "\n",
    "        dloss += loss.cpu().item()\n",
    "        dacc += (output==target).sum().item()\n",
    "        \n",
    "    train_loss.append(dloss/batch[0])\n",
    "    train_acc.append(dacc/(batch[0]*batch[1]))\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test(model, testX, testY, device, test_loss, test_acc):\n",
    "    model.eval()\n",
    "    loss, output, target = model(testX, testY)\n",
    "    loss = loss.sum()\n",
    "    \n",
    "    _, output = torch.max(output, 1)\n",
    "    _, target = torch.max(target, 1)\n",
    "    \n",
    "    test_loss.append(loss.cpu().item())\n",
    "    test_acc.append((output==target).sum().item()/len(testX))\n",
    "    return test_loss, test_acc, output, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]mic_20.0_model2\n",
      "mem data shape  -  20.0 ms\n",
      "train set : torch.Size([5000, 10, 200]) torch.Size([5000, 5])\n",
      "test set : torch.Size([1500, 10, 200]) torch.Size([1500, 5])\n",
      "[0. 0. 1. 0. 0.]\n",
      "epoch 0 - train loss : 0.8767160  /  test loss : 1.0009491\n",
      "           train acc : 0.4612000  /  test acc : 0.2000000\n",
      "[0.84       0.85       0.86666667 0.42333333 0.52333333]\n",
      "epoch 10 - train loss : 0.1032831  /  test loss : 0.6420491\n",
      "           train acc : 0.9720000  /  test acc : 0.7006667\n",
      "[0.97666667 0.72       0.86       0.69666667 0.67666667]\n",
      "epoch 20 - train loss : 0.0134282  /  test loss : 0.6263961\n",
      "           train acc : 0.9998000  /  test acc : 0.7860000\n",
      "[0.97       0.7        0.82666667 0.65666667 0.68666667]\n",
      "epoch 30 - train loss : 0.0045620  /  test loss : 0.6908612\n",
      "           train acc : 1.0000000  /  test acc : 0.7680000\n",
      "[0.97333333 0.69       0.83       0.67333333 0.68666667]\n",
      "epoch 40 - train loss : 0.0022149  /  test loss : 0.7350190\n",
      "           train acc : 1.0000000  /  test acc : 0.7706667\n",
      "[0.95666667 0.69       0.83       0.68       0.69      ]\n",
      "epoch 50 - train loss : 0.0014128  /  test loss : 0.7660338\n",
      "           train acc : 1.0000000  /  test acc : 0.7693333\n",
      "[0.96       0.69       0.83       0.68       0.68333333]\n",
      "epoch 60 - train loss : 0.0009654  /  test loss : 0.7896748\n",
      "           train acc : 1.0000000  /  test acc : 0.7686667\n",
      "[0.96333333 0.69       0.82       0.65666667 0.67666667]\n",
      "epoch 70 - train loss : 0.0008141  /  test loss : 0.8104635\n",
      "           train acc : 1.0000000  /  test acc : 0.7613333\n",
      "[0.95       0.69       0.82333333 0.67       0.68      ]\n",
      "epoch 80 - train loss : 0.0005623  /  test loss : 0.8301563\n",
      "           train acc : 1.0000000  /  test acc : 0.7626667\n",
      "[0.95666667 0.69       0.82333333 0.67       0.67666667]\n",
      "epoch 90 - train loss : 0.0004673  /  test loss : 0.8470588\n",
      "           train acc : 1.0000000  /  test acc : 0.7633333\n",
      "[0.95       0.68666667 0.82333333 0.68       0.68      ]\n",
      "epoch 100 - train loss : 0.0003745  /  test loss : 0.8581103\n",
      "           train acc : 1.0000000  /  test acc : 0.7640000\n",
      "[0.95333333 0.69666667 0.82333333 0.66       0.67666667]\n",
      "epoch 110 - train loss : 0.0003454  /  test loss : 0.8686943\n",
      "           train acc : 1.0000000  /  test acc : 0.7620000\n",
      "[0.96333333 0.68666667 0.82333333 0.67333333 0.69      ]\n",
      "epoch 120 - train loss : 0.0002982  /  test loss : 0.8804345\n",
      "           train acc : 1.0000000  /  test acc : 0.7673333\n",
      "[0.96333333 0.69333333 0.82333333 0.67666667 0.67333333]\n",
      "epoch 130 - train loss : 0.0002591  /  test loss : 0.8931706\n",
      "           train acc : 1.0000000  /  test acc : 0.7660000\n",
      "[0.97333333 0.68666667 0.81       0.67333333 0.68666667]\n",
      "epoch 140 - train loss : 0.0001895  /  test loss : 0.8970279\n",
      "           train acc : 1.0000000  /  test acc : 0.7660000\n",
      "[0.95333333 0.69666667 0.82333333 0.67666667 0.67      ]\n",
      "epoch 150 - train loss : 0.0001888  /  test loss : 0.9070189\n",
      "           train acc : 1.0000000  /  test acc : 0.7640000\n",
      "[0.95       0.68666667 0.82333333 0.67666667 0.66333333]\n",
      "epoch 160 - train loss : 0.0001698  /  test loss : 0.9185661\n",
      "           train acc : 1.0000000  /  test acc : 0.7600000\n",
      "[0.96       0.69666667 0.81666667 0.68333333 0.67666667]\n",
      "epoch 170 - train loss : 0.0001383  /  test loss : 0.9207224\n",
      "           train acc : 1.0000000  /  test acc : 0.7666667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8843774d2fca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-54bd42167e0a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainX, trainY, batch, device, optimizer, train_loss, train_acc)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mdacc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    print('[Training]%s_%.1f_model%d'%(data_type, time_len, model_num))\n",
    "    ch_list = [0,2,4,6,8,10,12,14,16,18]\n",
    "    torch.manual_seed(37)\n",
    "    torch.cuda.manual_seed_all(37)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    trainX = np.load('npy_data/%s/19ch_%.1fms_trainX.npy'%(data_type, time_len))[:,ch_list,:]\n",
    "    trainY = np.load('npy_data/%s/19ch_%.1fms_trainY.npy'%(data_type, time_len))\n",
    "    testX = np.load('npy_data/%s/19ch_%.1fms_testX.npy'%(data_type, time_len))[:,ch_list,:]\n",
    "    testY = np.load('npy_data/%s/19ch_%.1fms_testY.npy'%(data_type, time_len))\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    batch_num = 1000\n",
    "    batch = [int(len(trainX)/batch_num), batch_num]\n",
    "    \n",
    "    trainX = torch.Tensor(trainX).to(device)\n",
    "#     _, trainY = torch.max(torch.LongTensor(trainY).to(device), 1)\n",
    "    trainY = torch.Tensor(trainY).to(device)\n",
    "    testX = torch.Tensor(testX).to(device)\n",
    "    testY = torch.Tensor(testY).to(device)\n",
    "#     _, testY = torch.max(torch.LongTensor(testY).to(device), 1)\n",
    "    print('mem data shape  -  %.1f ms'%time_len)\n",
    "    print('train set :', np.shape(trainX) , np.shape(trainY))\n",
    "    print('test set :', np.shape(testX) ,np.shape(testY))\n",
    "    \n",
    "    \n",
    "    learning_rate = 0.0002\n",
    "    loss_func=nn.BCELoss()\n",
    "    model = nn.DataParallel(Model(loss_func)).to(device)\n",
    "#     model = Model(loss_func).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=batch_num, eta_min = 3e-6)\n",
    "\n",
    "#     model.load_state_dict(torch.load('ckpt/model%d/%s/%.1f_ckpt_100.pt'%(model_num, data_type, time_len)))\n",
    "    a = time.time()\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    for epoch in range(1001):\n",
    "        train_loss, train_acc = train(model, trainX, trainY, batch, device, optimizer, train_loss, train_acc)\n",
    "        test_loss, test_acc, output, target = test(model, testX, testY, device, test_loss, test_acc)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch%10==0:\n",
    "            vowel_check = (output==target).cpu().detach().numpy()\n",
    "            vowel_acc = np.zeros(5)\n",
    "            for k1 in range(15):\n",
    "                for k2 in range(5):\n",
    "                    vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "            print(vowel_acc/(vow_num*15))\n",
    "            print('epoch %d - train loss : %.7f  /  test loss : %.7f'%(epoch, train_loss[-1], test_loss[-1]))\n",
    "            print('           train acc : %.7f  /  test acc : %.7f'%(train_acc[-1], test_acc[-1]))\n",
    "#         if epoch%50==0:\n",
    "#             print('@@@@@@@ save model : epoch %d'% epoch)\n",
    "#             torch.save(model.state_dict(),'ckpt/model%d_%s/%.1f_ckpt_%d.pt'%(model_num, data_type, time_len, epoch))\n",
    "#             np.savetxt('result/model%d_%s/%.1f_loss_tr.txt'%(model_num, data_type, time_len), train_loss)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_loss_te.txt'%(model_num, data_type, time_len), test_loss)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_acc_tr.txt'%(model_num, data_type, time_len), train_acc)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_acc_te.txt'%(model_num, data_type, time_len), test_acc)\n",
    "\n",
    "    vowel_check = (output==target).cpu().detach().numpy()\n",
    "    vowel_acc = np.zeros(5)\n",
    "    for k1 in range(15):\n",
    "        for k2 in range(5):\n",
    "            vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "    print(vowel_acc/(vow_num*15))\n",
    "    print('epoch %d - train loss : %.7f  /  test loss : %.7f'%(epoch, train_loss[-1], test_loss[-1]))\n",
    "    print('           train acc : %.7f  /  test acc : %.7f'%(train_acc[-1], test_acc[-1]))\n",
    "    print(\"training complete! - calculation time :\", time.time()-a, '  seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.846\n",
      "0.8066666666666666\n",
      "0.8266666666666667\n",
      "0.8613333333333333\n",
      "677\n",
      "785\n",
      "269\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "print(np.max(np.loadtxt('result/model2/mfcc/20.0_acc_te.txt')))\n",
    "print(np.max(np.loadtxt('result/model2/raw/20.0_acc_te.txt')))\n",
    "print(np.max(np.loadtxt('result/model2/fft/20.0_acc_te.txt')))\n",
    "print(np.max(np.loadtxt('result/model2/mem/20.0_acc_te.txt')))\n",
    "print(np.argmax(np.loadtxt('result/model2/mfcc/20.0_acc_te.txt')))\n",
    "print(np.argmax(np.loadtxt('result/model2/raw/20.0_acc_te.txt')))\n",
    "print(np.argmax(np.loadtxt('result/model2/fft/20.0_acc_te.txt')))\n",
    "print(np.argmax(np.loadtxt('result/model2/mem/20.0_acc_te.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole accuracy : [0.95       0.69       0.82       0.67333333 0.67666667]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[2 2 3 3 2 2 2 3 3 3 2 2 2 3 2 2 2 2 2 2]\n",
      "[5 4 4 4 4 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[5 5 5 4 5 4 4 4 5 5 4 4 5 5 4 5 5 5 5 5]\n",
      "[4 4 5 4 4 5 4 4 5 5 5 5 4 5 5 4 4 4 5 5]\n",
      "[5 4 2 4 4 4 4 4 4 2 5 4 5 4 2 4 2 2 2 4]\n",
      "[4 4 5 4 4 4 4 4 5 5 5 4 5 4 4 4 4 4 4 4]\n",
      "[4 4 4 4 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[5 4 5 4 4 4 4 4 5 4 4 4 4 5 5 4 4 5 4 4]\n",
      "[4 4 4 4 5 4 4 5 4 4 5 5 4 5 4 4 4 4 4 4]\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[4 4 5 4 4 4 4 5 4 4 5 4 5 4 4 4 5 5 4 5]\n",
      "\n",
      "part accuracy \n",
      " {0.0: 0.6733333333333333, 2.0: 0.06666666666666667, 3.0: 0.08666666666666667, 5.0: 0.17333333333333334}\n"
     ]
    }
   ],
   "source": [
    "vowel_check = (output==target).cpu().detach().numpy()\n",
    "vowel_acc = np.zeros(5)\n",
    "for k1 in range(15):\n",
    "    for k2 in range(5):\n",
    "        vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "print('whole accuracy :', vowel_acc/(vow_num*15))\n",
    "            \n",
    "vowel_check = (output==target).cpu().detach().numpy()\n",
    "whole_count=np.array([])\n",
    "for k1 in range(15):\n",
    "    for k2 in range(5):\n",
    "        if k2==3:\n",
    "            aa = output[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)].cpu().detach().numpy()+1\n",
    "            print(aa)\n",
    "            bb = aa*((-1*np.int32(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)]))+1)\n",
    "            whole_count = np.append(whole_count,bb)\n",
    "unique, counts = np.unique(whole_count, return_counts=True)\n",
    "#         vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "print('\\npart accuracy \\n', dict(zip(unique, counts/(len(output)/5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
