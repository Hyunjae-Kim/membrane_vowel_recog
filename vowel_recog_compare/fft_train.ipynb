{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'fft'\n",
    "model_num = 2\n",
    "time_len =  20  ## ms\n",
    "if time_len==0.5:\n",
    "    point_len = 25\n",
    "else:\n",
    "    point_len = int(45000*time_len/1000)\n",
    "div_num = int(450*(200/point_len))\n",
    "vow_num = int(div_num/5)\n",
    "    \n",
    "if time_len==0.5:\n",
    "    point_len = 13\n",
    "else:\n",
    "    point_len = int(45000*time_len/1000/2)+1\n",
    "\n",
    "fc_len = int(point_len/32)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, loss):\n",
    "        super(Model, self).__init__()\n",
    "        prob = 0.8\n",
    "        input_c = 1\n",
    "        channel = 32\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_c, channel, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(channel, channel*2, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*2))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(channel*2, channel*2, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*2))\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(channel*2, channel*4, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*4))\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(channel*4, channel*4, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*4))\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(channel*4*fc_len, 1024),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(1024))\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 5),\n",
    "            nn.Softmax(dim=1))\n",
    "        self.loss = loss\n",
    "        \n",
    "    def forward(self, data, target):\n",
    "        x = self.conv1(data)\n",
    "#         print('1',x.size())\n",
    "        x = self.conv2(x)\n",
    "#         print('2',x.size())\n",
    "        x = self.conv3(x)\n",
    "#         print('3',x.size())\n",
    "        x = self.conv4(x)\n",
    "#         print('4',x.size())\n",
    "        x = self.conv5(x)\n",
    "        x = x.view(x.size()[0],-1)\n",
    "        x = self.fc1(x)\n",
    "#         print('8',x.size())\n",
    "        h = self.fc2(x)\n",
    "#         print('9',h.size())\n",
    "        \n",
    "        l = self.loss(h, target)\n",
    "        return l, h, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainX, trainY, batch, device, optimizer, train_loss, train_acc):\n",
    "    model.train()\n",
    "    dloss = 0\n",
    "    dacc = 0\n",
    "    \n",
    "    rand = torch.randperm(trainX.size()[0])\n",
    "    trainX = trainX[rand]\n",
    "    trainY = trainY[rand]\n",
    "    \n",
    "    for i in range(batch[0]):\n",
    "        optimizer.zero_grad()\n",
    "        loss, output, target = model(trainX[i*batch[1]:(i+1)*batch[1]], trainY[i*batch[1]:(i+1)*batch[1]])\n",
    "        loss = loss.sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, output = torch.max(output, 1)\n",
    "        _, target = torch.max(target, 1)\n",
    "\n",
    "        dloss += loss.cpu().item()\n",
    "        dacc += (output==target).sum().item()\n",
    "        \n",
    "    train_loss.append(dloss/batch[0])\n",
    "    train_acc.append(dacc/(batch[0]*batch[1]))\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test(model, testX, testY, device, test_loss, test_acc):\n",
    "    model.eval()\n",
    "    loss, output, target = model(testX, testY)\n",
    "    loss = loss.sum()\n",
    "    \n",
    "    _, output = torch.max(output, 1)\n",
    "    _, target = torch.max(target, 1)\n",
    "    \n",
    "    test_loss.append(loss.cpu().item())\n",
    "    test_acc.append((output==target).sum().item()/len(testX))\n",
    "    return test_loss, test_acc, output, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]fft_20.0_model2\n",
      "fft data shape  -  20.0 ms\n",
      "train set : torch.Size([5000, 1, 451]) torch.Size([5000, 5])\n",
      "test set : torch.Size([1500, 1, 451]) torch.Size([1500, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/khj/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1.]\n",
      "epoch 0 - train loss : 0.9135068  /  test loss : 1.0012305\n",
      "           train acc : 0.4448000  /  test acc : 0.2000000\n",
      "[0.12 0.23 1.   0.   0.  ]\n",
      "epoch 10 - train loss : 0.1810287  /  test loss : 1.0932188\n",
      "           train acc : 0.9500000  /  test acc : 0.2700000\n",
      "[0.94666667 0.72666667 0.93333333 0.63333333 0.61666667]\n",
      "epoch 20 - train loss : 0.0619643  /  test loss : 0.4306512\n",
      "           train acc : 0.9904000  /  test acc : 0.7713333\n",
      "[0.95       0.75333333 0.93333333 0.67       0.70333333]\n",
      "epoch 30 - train loss : 0.0280358  /  test loss : 0.4070527\n",
      "           train acc : 0.9970000  /  test acc : 0.8020000\n",
      "[0.95       0.77       0.93333333 0.68333333 0.72666667]\n",
      "epoch 40 - train loss : 0.0157104  /  test loss : 0.4133573\n",
      "           train acc : 0.9990000  /  test acc : 0.8126667\n",
      "[0.94333333 0.76666667 0.93333333 0.72       0.73666667]\n",
      "epoch 50 - train loss : 0.0093698  /  test loss : 0.4092107\n",
      "           train acc : 0.9992000  /  test acc : 0.8200000\n",
      "[0.94333333 0.77666667 0.93333333 0.69333333 0.72666667]\n",
      "epoch 60 - train loss : 0.0065872  /  test loss : 0.4206222\n",
      "           train acc : 1.0000000  /  test acc : 0.8146667\n",
      "[0.94       0.76666667 0.93333333 0.71666667 0.73      ]\n",
      "epoch 70 - train loss : 0.0046486  /  test loss : 0.4208313\n",
      "           train acc : 1.0000000  /  test acc : 0.8173333\n",
      "[0.93666667 0.78       0.93333333 0.70666667 0.73333333]\n",
      "epoch 80 - train loss : 0.0034453  /  test loss : 0.4316320\n",
      "           train acc : 1.0000000  /  test acc : 0.8180000\n",
      "[0.93666667 0.78       0.93333333 0.71666667 0.72666667]\n",
      "epoch 90 - train loss : 0.0027467  /  test loss : 0.4296910\n",
      "           train acc : 1.0000000  /  test acc : 0.8186667\n",
      "[0.93333333 0.77666667 0.93333333 0.72333333 0.72      ]\n",
      "epoch 100 - train loss : 0.0022423  /  test loss : 0.4418070\n",
      "           train acc : 1.0000000  /  test acc : 0.8173333\n",
      "[0.93333333 0.77333333 0.93333333 0.72666667 0.72666667]\n",
      "epoch 110 - train loss : 0.0018075  /  test loss : 0.4439297\n",
      "           train acc : 1.0000000  /  test acc : 0.8186667\n",
      "[0.93333333 0.77666667 0.93333333 0.71666667 0.72      ]\n",
      "epoch 120 - train loss : 0.0015727  /  test loss : 0.4447833\n",
      "           train acc : 1.0000000  /  test acc : 0.8160000\n",
      "[0.93333333 0.77       0.93333333 0.73666667 0.72333333]\n",
      "epoch 130 - train loss : 0.0013582  /  test loss : 0.4459147\n",
      "           train acc : 1.0000000  /  test acc : 0.8193333\n",
      "[0.93333333 0.77666667 0.93333333 0.72666667 0.72      ]\n",
      "epoch 140 - train loss : 0.0011779  /  test loss : 0.4539746\n",
      "           train acc : 1.0000000  /  test acc : 0.8180000\n",
      "[0.93333333 0.78333333 0.93333333 0.71666667 0.72666667]\n",
      "epoch 150 - train loss : 0.0011128  /  test loss : 0.4590819\n",
      "           train acc : 1.0000000  /  test acc : 0.8186667\n",
      "[0.93333333 0.77666667 0.93333333 0.72333333 0.72666667]\n",
      "epoch 160 - train loss : 0.0009783  /  test loss : 0.4595028\n",
      "           train acc : 1.0000000  /  test acc : 0.8186667\n",
      "[0.93333333 0.77666667 0.93333333 0.73666667 0.71666667]\n",
      "epoch 170 - train loss : 0.0008408  /  test loss : 0.4645086\n",
      "           train acc : 1.0000000  /  test acc : 0.8193333\n",
      "[0.93333333 0.78666667 0.93333333 0.73333333 0.72      ]\n",
      "epoch 180 - train loss : 0.0007361  /  test loss : 0.4630321\n",
      "           train acc : 1.0000000  /  test acc : 0.8213333\n",
      "[0.93333333 0.78333333 0.93333333 0.73       0.72666667]\n",
      "epoch 190 - train loss : 0.0006619  /  test loss : 0.4694255\n",
      "           train acc : 1.0000000  /  test acc : 0.8213333\n",
      "[0.93333333 0.78       0.93333333 0.73333333 0.71      ]\n",
      "epoch 200 - train loss : 0.0005993  /  test loss : 0.4738963\n",
      "           train acc : 1.0000000  /  test acc : 0.8180000\n",
      "[0.93333333 0.78       0.93333333 0.74333333 0.72666667]\n",
      "epoch 210 - train loss : 0.0005569  /  test loss : 0.4711885\n",
      "           train acc : 1.0000000  /  test acc : 0.8233333\n",
      "[0.93333333 0.78333333 0.93333333 0.74666667 0.71      ]\n",
      "epoch 220 - train loss : 0.0005064  /  test loss : 0.4792073\n",
      "           train acc : 1.0000000  /  test acc : 0.8213333\n",
      "[0.93666667 0.78666667 0.93333333 0.73333333 0.71333333]\n",
      "epoch 230 - train loss : 0.0004835  /  test loss : 0.4761458\n",
      "           train acc : 1.0000000  /  test acc : 0.8206667\n",
      "[0.93333333 0.78333333 0.93333333 0.73333333 0.71333333]\n",
      "epoch 240 - train loss : 0.0004471  /  test loss : 0.4831721\n",
      "           train acc : 1.0000000  /  test acc : 0.8193333\n",
      "[0.93666667 0.78333333 0.93333333 0.74       0.71      ]\n",
      "epoch 250 - train loss : 0.0003999  /  test loss : 0.4824861\n",
      "           train acc : 1.0000000  /  test acc : 0.8206667\n",
      "[0.93333333 0.79       0.93333333 0.72666667 0.74      ]\n",
      "epoch 260 - train loss : 0.0003775  /  test loss : 0.4790878\n",
      "           train acc : 1.0000000  /  test acc : 0.8246667\n",
      "training complete! - calculation time : 156.82037925720215   seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':    \n",
    "    print('[Training]%s_%.1f_model%d'%(data_type, time_len, model_num))\n",
    "    torch.manual_seed(37)\n",
    "    torch.cuda.manual_seed_all(37)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    trainX = np.load('npy_data/%s/%s_%.1fms_trainX.npy'%(data_type, data_type, time_len)).reshape(-1, 1, point_len)\n",
    "    trainY = np.load('npy_data/%s/%s_%.1fms_trainY.npy'%(data_type, data_type, time_len))\n",
    "    testX = np.load('npy_data/%s/%s_%.1fms_testX.npy'%(data_type, data_type, time_len)).reshape(-1, 1, point_len)\n",
    "    testY = np.load('npy_data/%s/%s_%.1fms_testY.npy'%(data_type, data_type, time_len))\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    batch_num = 1000\n",
    "    batch = [int(len(trainX)/batch_num), batch_num]\n",
    "    \n",
    "    trainX = torch.Tensor(trainX).to(device)\n",
    "    trainY = torch.Tensor(trainY).to(device)\n",
    "    testX = torch.Tensor(testX).to(device)\n",
    "    testY = torch.Tensor(testY).to(device)\n",
    "    print('%s data shape  -  %.1f ms'%(data_type, time_len))\n",
    "    print('train set :', np.shape(trainX) , np.shape(trainY))\n",
    "    print('test set :', np.shape(testX) ,np.shape(testY))\n",
    "    \n",
    "    \n",
    "    learning_rate = 0.00005\n",
    "    loss_func=nn.BCELoss()\n",
    "    model = nn.DataParallel(Model(loss_func)).to(device)\n",
    "#     model = Model(loss_func).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=batch_num, eta_min = 3e-6)\n",
    "\n",
    "#     model.load_state_dict(torch.load('ckpt/model%d/%s/%.1f_ckpt_1000.pt'%(model_num, data_type, time_len)))\n",
    "    a = time.time()\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    for epoch in range(270):\n",
    "        train_loss, train_acc = train(model, trainX, trainY, batch, device, optimizer, train_loss, train_acc)\n",
    "        test_loss, test_acc, output, target = test(model, testX, testY, device, test_loss, test_acc)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch%10==0: \n",
    "            vowel_check = (output==target).cpu().detach().numpy()\n",
    "            vowel_acc = np.zeros(5)\n",
    "            for k1 in range(15):\n",
    "                for k2 in range(5):\n",
    "                    vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "            print(vowel_acc/(vow_num*15))\n",
    "            print('epoch %d - train loss : %.7f  /  test loss : %.7f'%(epoch, train_loss[-1], test_loss[-1]))\n",
    "            print('           train acc : %.7f  /  test acc : %.7f'%(train_acc[-1], test_acc[-1]))\n",
    "#         if epoch%50==0:\n",
    "#             print('@@@@@@@ save model : epoch %d'% epoch)\n",
    "#             torch.save(model.state_dict(),'ckpt/model%d_%s/%.1f_ckpt_%d.pt'%(model_num, data_type, time_len, epoch))\n",
    "#             np.savetxt('result/model%d_%s/%.1f_loss_tr.txt'%(model_num, data_type, time_len), train_loss)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_loss_te.txt'%(model_num, data_type, time_len), test_loss)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_acc_tr.txt'%(model_num, data_type, time_len), train_acc)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_acc_te.txt'%(model_num, data_type, time_len), test_acc)\n",
    "    print(\"training complete! - calculation time :\", time.time()-a, '  seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole accuracy : [0.93666667 0.80666667 0.93       0.73       0.73      ]\n",
      "\n",
      "part accuracy \n",
      " {0.0: 0.73, 2.0: 0.013333333333333334, 3.0: 0.043333333333333335, 4.0: 0.21333333333333335}\n"
     ]
    }
   ],
   "source": [
    "vowel_check = (output==target).cpu().detach().numpy()\n",
    "vowel_acc = np.zeros(5)\n",
    "for k1 in range(15):\n",
    "    for k2 in range(5):\n",
    "        vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "print('whole accuracy :', vowel_acc/(vow_num*15))\n",
    "            \n",
    "vowel_check = (output==target).cpu().detach().numpy()\n",
    "whole_count=np.array([])\n",
    "for k1 in range(15):\n",
    "    for k2 in range(5):\n",
    "        if k2==4:\n",
    "            aa = output[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)].cpu().detach().numpy()+1\n",
    "#             print(aa)\n",
    "            bb = aa*((-1*np.int32(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)]))+1)\n",
    "            whole_count = np.append(whole_count,bb)\n",
    "unique, counts = np.unique(whole_count, return_counts=True)\n",
    "#         vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "print('\\npart accuracy \\n', dict(zip(unique, counts/(len(output)/5))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
