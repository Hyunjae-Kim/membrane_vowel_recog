{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "devices = device_lib.list_local_devices()\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "np.random.seed(37)\n",
    "# flags = tf.app.flags\n",
    "# flags.DEFINE_integer(\"pt_len\",100,\"input size rag\")\n",
    "# flags.DEFINE_integer(\"model_num\",5,\"network model number\")\n",
    "# FLAGS = flags.FLAGS\n",
    "                     \n",
    "seed_num = 137\n",
    "model_num = 1\n",
    "\n",
    "point_num = 10\n",
    "# point_num = 1\n",
    "# point_length = FLAGS.pt_len\n",
    "point_length = 200\n",
    "##sec_div = int(200/point_length)\n",
    "input_size = int(point_length*point_num)\n",
    "\n",
    "vowel_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset : (5000, 2000) (1500, 2000)\n",
      "labels : (5000, 5) (1500, 5)\n",
      "NN membrane dataset : (5000, 200, 10) (1500, 200, 10)\n"
     ]
    }
   ],
   "source": [
    "if vowel_num==3:\n",
    "    st = ''\n",
    "elif vowel_num==4:\n",
    "    st = 'o'\n",
    "elif vowel_num==5:\n",
    "    st = 'ou'\n",
    "    \n",
    "# trainX = np.load('data/20181226_2/%dc_aei%s_%dp_trainX.npy'%(point_num,st,point_length)).reshape(5000,-1)\n",
    "trainX = np.load('data/20181226_2/%dc_aei%s_%dp_trainX.npy'%(point_num,st,point_length)).transpose(0,2,1)\n",
    "trainY = np.load('data/20181226_2/%dc_aei%s_%dp_trainY.npy'%(point_num,st,point_length))\n",
    "# testX = np.load('data/20181226_2/%dc_aei%s_%dp_testX.npy'%(point_num,st,point_length)).reshape(1500,-1)\n",
    "testX = np.load('data/20181226_2/%dc_aei%s_%dp_testX.npy'%(point_num,st,point_length)).transpose(0,2,1)\n",
    "testY = np.load('data/20181226_2/%dc_aei%s_%dp_testY.npy'%(point_num,st,point_length))\n",
    "\n",
    "mem_trX = np.transpose(np.load('NNmem/mem_data3/aeiou200len_tr.npy'), (0,2,1))\n",
    "mem_teX = np.transpose(np.load('NNmem/mem_data3/aeiou200len_te.npy'), (0,2,1))\n",
    "\n",
    "######noise bias\n",
    "trX = trainX-np.mean(trainX,2).reshape(-1,10,1)\n",
    "trX_list = np.sqrt(np.mean(np.square(trX),2)).reshape(-1,10,1)>0.02\n",
    "teX = testX-np.mean(testX,2).reshape(-1,10,1)\n",
    "teX_list = np.sqrt(np.mean(np.square(teX),2)).reshape(-1,10,1)>0.02\n",
    "\n",
    "trainX = (trX_list*trainX).transpose(0,2,1).reshape(5000,-1)\n",
    "testX = (teX_list*testX).transpose(0,2,1).reshape(1500,-1)\n",
    "\n",
    "print('original dataset :', np.shape(trainX), np.shape(testX))\n",
    "print('labels :', np.shape(trainY), np.shape(testY))\n",
    "print('NN membrane dataset :', np.shape(mem_trX), np.shape(mem_teX))\n",
    "\n",
    "# trainX = mem_trX.reshape(5000,-1)\n",
    "# testX = mem_teX.reshape(1500,-1)\n",
    "\n",
    "\n",
    "#########check dataset order... exactly same  -->  label can use directly\n",
    "# mem_trX = np.load('mem_data3/aeiou200len_tr.npy')\n",
    "# mem_teX = np.load('mem_data3/aeiou200len_te.npy')\n",
    "# trainX = np.transpose(trainX, (0,2,1))\n",
    "\n",
    "# trX = trainX-np.mean(trainX,2).reshape(-1,10,1)\n",
    "# trX_list = np.sqrt(np.mean(np.square(trX),2)).reshape(-1,10,1)>0.02\n",
    "# trainX = trainX*trX_list\n",
    "\n",
    "# count = 0\n",
    "# for k1 in range(len(mem_trX)):\n",
    "#     for k2 in range(len(mem_trX[0])):\n",
    "#         if np.mean(np.square(mem_trX[k1][k2]-trainX[k1][k2]))>0.0003:\n",
    "#             count += 1\n",
    "#             plt.plot(mem_trX[k1][k2], 'r')\n",
    "#             plt.plot(trainX[k1][k2], 'k')\n",
    "#             plt.axis([0,200, -0.5, 0.5]) \n",
    "#             plt.show()\n",
    "# print('fin', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Run]ori_nosX_aeiou_1_200point\n",
      "filter : [16, 32, 64] filter size : 1x5\n",
      "Training data : (5000, 2000)\n",
      "Test data : (1500, 2000)\n",
      "data length : 2000\n"
     ]
    }
   ],
   "source": [
    "epoch = 2001\n",
    "l_rate = 0.0003\n",
    "conv_std = 2\n",
    "\n",
    "model_name = \"ori_nosX_aei%s_%d_%dpoint\"%(st,model_num, point_length)\n",
    "print(\"\\n[Run]%s\"%model_name)\n",
    "\n",
    "if model_num == 1:  ld = 3; filt_size = [16, 32, 64]; filt_shape = 5\n",
    "elif model_num ==2: ld = 4; filt_size = [16, 32, 64, 128]; filt_shape = 5\n",
    "elif model_num ==3: ld = 5; filt_size = [16, 16, 32, 64, 128]; filt_shape = 5\n",
    "elif model_num ==4: ld = 6; filt_size = [16, 16, 32, 64, 64, 128]; filt_shape = 5\n",
    "elif model_num ==5: ld = 7; filt_size = [16, 16, 32, 32, 64, 128, 128]; filt_shape = 5\n",
    "##elif model_num ==6: ld = 10; filt_size = [16, 16, 32, 32, 64, 64, 128, 128, 256, 256]; filt_shape = 5\n",
    "print(\"filter :\", filt_size, \"filter size : 1x%d\"%filt_shape)\n",
    "print(\"Training data :\", np.shape(trainX))\n",
    "print(\"Test data :\", np.shape(testX))\n",
    "print(\"data length :\", len(testX[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1, seed=seed_num)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, conv_std, 1], padding='SAME')\n",
    "\n",
    "def leaky_relu(x, alpha=0.2):\n",
    "    output = tf.maximum(x, alpha * x)\n",
    "    return output\n",
    "\n",
    "def layer_set(layer_num, x_in, w_in, w_out, keep_prob):\n",
    "    with tf.variable_scope('conv_%d'%layer_num):\n",
    "        W_conv = weight_variable([1,filt_shape,w_in,w_out])\n",
    "        b_conv = bias_variable([w_out])\n",
    "        z_conv = conv2d(x_in, W_conv) + b_conv\n",
    "        do_conv = tf.nn.dropout(z_conv, keep_prob=keep_prob)\n",
    "        a_conv = leaky_relu(do_conv)\n",
    "        shape = a_conv.get_shape()\n",
    "        print(\"layer%d shape :\"%layer_num, shape)\n",
    "    return a_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 200, 10)\n",
      "layer1 shape : (?, 1, 100, 16)\n",
      "layer2 shape : (?, 1, 50, 32)\n",
      "layer3 shape : (?, 1, 25, 64)\n"
     ]
    }
   ],
   "source": [
    "##X = tf.placeholder(shape=[None, point_length, point_num], dtype= tf.float32)\n",
    "X = tf.placeholder(shape=[None, input_size], dtype= tf.float32)\n",
    "Y = tf.placeholder(shape=[None, vowel_num], dtype= tf.float32)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "##x_image = tf.reshape(X, [-1,1,point_length,point_num])\n",
    "x_image = tf.reshape(X, [-1,1,point_length, point_num])\n",
    "print(np.shape(x_image))\n",
    "\n",
    "if ld == 3:\n",
    "    out1 = layer_set(1,x_image, point_num, filt_size[0], keep_prob)\n",
    "    out2 = layer_set(2,out1, filt_size[0], filt_size[1], keep_prob)\n",
    "    out_last = layer_set(3,out2, filt_size[1], filt_size[2], keep_prob)\n",
    "elif ld == 4:\n",
    "    out1 = layer_set(1,x_image, point_num, filt_size[0], keep_prob)\n",
    "    out2 = layer_set(2,out1, filt_size[0], filt_size[1], keep_prob)\n",
    "    out3 = layer_set(3,out2, filt_size[1], filt_size[2], keep_prob)\n",
    "    out_last = layer_set(4,out3, filt_size[2], filt_size[3], keep_prob)\n",
    "elif ld ==5:\n",
    "    out1 = layer_set(1,x_image, point_num, filt_size[0], keep_prob)\n",
    "    out2 = layer_set(2,out1, filt_size[0], filt_size[1], keep_prob)\n",
    "    out3 = layer_set(3,out2, filt_size[1], filt_size[2], keep_prob)\n",
    "    out4 = layer_set(4,out3, filt_size[2], filt_size[3], keep_prob)\n",
    "    out_last = layer_set(5,out4, filt_size[3], filt_size[4], keep_prob)\n",
    "elif ld ==6:\n",
    "    out1 = layer_set(1,x_image, point_num, filt_size[0], keep_prob)\n",
    "    out2 = layer_set(2,out1, filt_size[0], filt_size[1], keep_prob)\n",
    "    out3 = layer_set(3,out2, filt_size[1], filt_size[2], keep_prob)\n",
    "    out4 = layer_set(4,out3, filt_size[2], filt_size[3], keep_prob)\n",
    "    out5 = layer_set(5,out4, filt_size[3], filt_size[4], keep_prob)\n",
    "    out_last = layer_set(6,out5, filt_size[4], filt_size[5], keep_prob)\n",
    "elif ld ==7:\n",
    "    out1 = layer_set(1,x_image, point_num, filt_size[0], keep_prob)\n",
    "    out2 = layer_set(2,out1, filt_size[0], filt_size[1], keep_prob)\n",
    "    out3 = layer_set(3,out2, filt_size[1], filt_size[2], keep_prob)\n",
    "    out4 = layer_set(4,out3, filt_size[2], filt_size[3], keep_prob)\n",
    "    out5 = layer_set(5,out4, filt_size[3], filt_size[4], keep_prob)\n",
    "    out6 = layer_set(6,out5, filt_size[4], filt_size[5], keep_prob)\n",
    "    out_last = layer_set(7,out6, filt_size[5], filt_size[6], keep_prob)\n",
    "elif ld ==10:\n",
    "    out1 = layer_set(1,x_image, point_num, filt_size[0], keep_prob)\n",
    "    out2 = layer_set(2,out1, filt_size[0], filt_size[1], keep_prob)\n",
    "    out3 = layer_set(3,out2, filt_size[1], filt_size[2], keep_prob)\n",
    "    out4 = layer_set(4,out3, filt_size[2], filt_size[3], keep_prob)\n",
    "    out5 = layer_set(5,out4, filt_size[3], filt_size[4], keep_prob)\n",
    "    out6 = layer_set(6,out5, filt_size[4], filt_size[5], keep_prob)\n",
    "    out7 = layer_set(7,out6, filt_size[5], filt_size[6], keep_prob)\n",
    "    out8 = layer_set(8,out7, filt_size[6], filt_size[7], keep_prob)\n",
    "    out9 = layer_set(9,out8, filt_size[7], filt_size[8], keep_prob)\n",
    "    out_last = layer_set(10,out9, filt_size[8], filt_size[9], keep_prob)\n",
    "    \n",
    "out_shape = out_last.get_shape()\n",
    "out_last_size = int(out_shape[1]*out_shape[2]*out_shape[3])\n",
    "\n",
    "W_fc1 = weight_variable([out_last_size, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "out_flat = tf.reshape(out_last, [-1, out_last_size])\n",
    "z_fc1 = tf.nn.dropout(tf.matmul(out_flat, W_fc1) + b_fc1, keep_prob=keep_prob)\n",
    "a_fc1 = tf.nn.relu(z_fc1)\n",
    "\n",
    "W_fc2 = weight_variable([1024,vowel_num])\n",
    "b_fc2 = bias_variable([vowel_num])\n",
    "\n",
    "_logit = tf.matmul(a_fc1, W_fc2)+b_fc2\n",
    "_y = tf.nn.softmax(_logit)\n",
    "\n",
    "##loss = tf.losses.mean_squared_error(labels = Y, predictions = _logit)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=_logit, labels = Y))\n",
    "train = tf.train.AdamOptimizer(l_rate).minimize(loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(_y, 1), tf.argmax(Y,1))\n",
    "cast_ans = tf.cast(correct_pred, tf.float32)\n",
    "accuracy = tf.reduce_mean(cast_ans)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 2.22223701477 train accuracy: 0.193 test accuracy: 0.2\n",
      "100 loss: 0.335226738453 train accuracy: 0.878 test accuracy: 0.815\n",
      "200 loss: 0.108329758048 train accuracy: 0.966 test accuracy: 0.886\n",
      "300 loss: 0.0484834872186 train accuracy: 0.987 test accuracy: 0.876\n",
      "400 loss: 0.0291300024837 train accuracy: 0.993 test accuracy: 0.885\n",
      "500 loss: 0.020721484907 train accuracy: 0.995 test accuracy: 0.879\n",
      "600 loss: 0.0142160411924 train accuracy: 0.996 test accuracy: 0.886\n",
      "700 loss: 0.0128367948346 train accuracy: 0.997 test accuracy: 0.886\n",
      "800 loss: 0.0106173326261 train accuracy: 0.998 test accuracy: 0.885\n",
      "900 loss: 0.0103836410679 train accuracy: 0.998 test accuracy: 0.885\n",
      "1000 loss: 0.00950906202197 train accuracy: 0.998 test accuracy: 0.883\n",
      "1100 loss: 0.00967889735475 train accuracy: 0.998 test accuracy: 0.877\n",
      "1200 loss: 0.0093513994012 train accuracy: 0.998 test accuracy: 0.883\n",
      "1300 loss: 0.00858700945973 train accuracy: 0.998 test accuracy: 0.887\n",
      "1400 loss: 0.008086274378 train accuracy: 0.998 test accuracy: 0.88\n",
      "1500 loss: 0.00864079589956 train accuracy: 0.998 test accuracy: 0.888\n",
      "1600 loss: 0.00887668556534 train accuracy: 0.998 test accuracy: 0.886\n",
      "1700 loss: 0.00856803008355 train accuracy: 0.998 test accuracy: 0.88\n",
      "1800 loss: 0.00836472399533 train accuracy: 0.998 test accuracy: 0.883\n",
      "1900 loss: 0.00830239038914 train accuracy: 0.998 test accuracy: 0.886\n",
      "2000 loss: 0.00786847611889 train accuracy: 0.998 test accuracy: 0.887\n",
      "a : 0.983333333333 e : 0.636666666667 i : 1.0 o : 0.813333333333 u : 1.0\n",
      "Running time : 330.88 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tr_data_len = np.shape(trainX)[0]\n",
    "te_data_len = np.shape(testX)[0]\n",
    "mini_batch_size = 1000\n",
    "test_batch_size = int(te_data_len/10)\n",
    "mini_batch_div_tr = int(tr_data_len/mini_batch_size)\n",
    "mini_batch_div_te = int(te_data_len/test_batch_size)\n",
    "\n",
    "#################### Train & Test\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "##\n",
    "##    saver.restore(sess, \"ckpt/[last]%s.ckpt\"%model_name)\n",
    "##    \n",
    "    L = []\n",
    "    Acc_train =[]\n",
    "    Acc_test = []\n",
    "    for step in range(epoch):\n",
    "        s_tr = np.arange(tr_data_len)\n",
    "        np.random.seed(seed_num)\n",
    "        np.random.shuffle(s_tr)\n",
    "\n",
    "        trainX = trainX[s_tr]\n",
    "        trainY = trainY[s_tr]\n",
    "\n",
    "        acc_tr_av = 0\n",
    "        acc_te_av = 0\n",
    "\n",
    "        L_tr = 0\n",
    "\n",
    "        cast_check = np.array([])\n",
    "        for mb in range(mini_batch_div_tr):\n",
    "            _, l_tr, acc_tr = sess.run([train, loss, accuracy],\n",
    "                                       feed_dict={X:trainX[mini_batch_size*mb:mini_batch_size*(mb+1)],\n",
    "                                                  Y:trainY[mini_batch_size*mb:mini_batch_size*(mb+1)],\n",
    "                                                  keep_prob:0.8})\n",
    "\n",
    "            acc_tr_av += acc_tr\n",
    "            L_tr += l_tr\n",
    "            \n",
    "        for mb in range(mini_batch_div_te):\n",
    "            l_te, acc_te, castA = sess.run([loss, accuracy, cast_ans],\n",
    "                                    feed_dict={X:testX[test_batch_size*mb:test_batch_size*(mb+1)],\n",
    "                                               Y:testY[test_batch_size*mb:test_batch_size*(mb+1)],\n",
    "                                               keep_prob:1.0})\n",
    "            acc_te_av += acc_te\n",
    "            cast_check = np.append(cast_check, castA)\n",
    "\n",
    "        acc_tr_av /= mini_batch_div_tr\n",
    "        acc_te_av /= mini_batch_div_te\n",
    "        L_tr /= mini_batch_div_tr\n",
    "        \n",
    "        L.append(L_tr)\n",
    "        Acc_train.append(acc_tr_av)\n",
    "        Acc_test.append(acc_te_av)\n",
    "        if step%100==0:\n",
    "            print(step, 'loss:' , L_tr , \"train accuracy:\", round(acc_tr_av,3), \"test accuracy:\", round(acc_te_av,3))\n",
    "        if step%500==0:\n",
    "            save_path = saver.save(sess, \"NNmem/ckpt/%s.ckpt\"%model_name)\n",
    "\n",
    "acc_A = 0\n",
    "acc_E = 0\n",
    "acc_I = 0\n",
    "acc_O = 0\n",
    "acc_U = 0\n",
    "te_seg_num = int(len(testX)/15)    ## 15 : # of people in testdata\n",
    "vow_seg_num = int(te_seg_num/vowel_num)\n",
    "for k in range(15):\n",
    "    acc_A += np.mean(cast_check[te_seg_num*k+vow_seg_num*0:te_seg_num*k+vow_seg_num*1])/15\n",
    "    acc_E += np.mean(cast_check[te_seg_num*k+vow_seg_num*1:te_seg_num*k+vow_seg_num*2])/15\n",
    "    acc_I += np.mean(cast_check[te_seg_num*k+vow_seg_num*2:te_seg_num*k+vow_seg_num*3])/15\n",
    "    if vowel_num==4:\n",
    "        acc_O += np.mean(cast_check[te_seg_num*k+vow_seg_num*3:te_seg_num*k+vow_seg_num*4])/15\n",
    "    if vowel_num==5:\n",
    "        acc_O += np.mean(cast_check[te_seg_num*k+vow_seg_num*3:te_seg_num*k+vow_seg_num*4])/15\n",
    "        acc_U += np.mean(cast_check[te_seg_num*k+vow_seg_num*4:te_seg_num*k+vow_seg_num*5])/15\n",
    "\n",
    "if vowel_num==3:\n",
    "    print('a :', acc_A, 'e :', acc_E, 'i :', acc_I)\n",
    "elif vowel_num==4:\n",
    "    print('a :', acc_A, 'e :', acc_E, 'i :', acc_I, 'o :', acc_O)\n",
    "elif vowel_num==5:\n",
    "    print('a :', acc_A, 'e :', acc_E, 'i :', acc_I, 'o :', acc_O, 'u :', acc_U)\n",
    "         \n",
    "f1=open('NNmem/result/%s_tr.txt'%model_name,'w')\n",
    "f2=open('NNmem/result/%s_te.txt'%model_name,'w')\n",
    "for k1 in range(len(Acc_train)):\n",
    "    f1.write('\\n%f'%Acc_train[k1])\n",
    "    f2.write('\\n%f'%Acc_test[k1])\n",
    "f1.close()\n",
    "f2.close()\n",
    "           \n",
    "end = timeit.default_timer()\n",
    "print(\"Running time :\", round((end - start),2), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
