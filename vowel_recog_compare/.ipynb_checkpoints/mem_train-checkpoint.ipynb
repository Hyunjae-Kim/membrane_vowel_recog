{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'mem'\n",
    "model_num = 2\n",
    "time_len =  20  ## ms\n",
    "point_len = int(time_len*10)\n",
    "div_num = int(100*(20/time_len))\n",
    "vow_num = int(div_num/5)\n",
    "\n",
    "fc_len = int(point_len/32)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, loss):\n",
    "        super(Model, self).__init__()\n",
    "        prob = 0.8\n",
    "        input_c = 10\n",
    "        channel = 32\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_c, channel, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(channel, channel*2, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*2))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(channel*2, channel*2, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*2))\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(channel*2, channel*4, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*4))\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(channel*4, channel*4, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*4))\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(channel*4*fc_len, 1024),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(1024))\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 5),\n",
    "            nn.Softmax(dim=1))\n",
    "        self.loss = loss\n",
    "        \n",
    "    def forward(self, data, target):\n",
    "        x = self.conv1(data)\n",
    "#         print('1',x.size())\n",
    "        x = self.conv2(x)\n",
    "#         print('2',x.size())\n",
    "        x = self.conv3(x)\n",
    "#         print('3',x.size())\n",
    "        x = self.conv4(x)\n",
    "#         print('4',x.size())\n",
    "        x = self.conv5(x)\n",
    "#         print('5',x.size())\n",
    "        x = x.view(x.size()[0],-1)\n",
    "        x = self.fc1(x)\n",
    "#         print('8',x.size())\n",
    "        h = self.fc2(x)\n",
    "#         print('9',h.size())\n",
    "        \n",
    "        l = self.loss(h, target)\n",
    "        return l, h, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainX, trainY, batch, device, optimizer, train_loss, train_acc):\n",
    "    model.train()\n",
    "    dloss = 0\n",
    "    dacc = 0\n",
    "    \n",
    "    rand = torch.randperm(trainX.size()[0])\n",
    "    trainX = trainX[rand]\n",
    "    trainY = trainY[rand]\n",
    "    \n",
    "    for i in range(batch[0]):\n",
    "        optimizer.zero_grad()\n",
    "        loss, output, target = model(trainX[i*batch[1]:(i+1)*batch[1]], trainY[i*batch[1]:(i+1)*batch[1]])\n",
    "        loss = loss.sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, output = torch.max(output, 1)\n",
    "        _, target = torch.max(target, 1)\n",
    "\n",
    "        dloss += loss.cpu().item()\n",
    "        dacc += (output==target).sum().item()\n",
    "        \n",
    "    train_loss.append(dloss/batch[0])\n",
    "    train_acc.append(dacc/(batch[0]*batch[1]))\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test(model, testX, testY, device, test_loss, test_acc):\n",
    "    model.eval()\n",
    "    loss, output, target = model(testX, testY)\n",
    "    loss = loss.sum()\n",
    "    \n",
    "    _, output = torch.max(output, 1)\n",
    "    _, target = torch.max(target, 1)\n",
    "    \n",
    "    test_loss.append(loss.cpu().item())\n",
    "    test_acc.append((output==target).sum().item()/len(testX))\n",
    "    return test_loss, test_acc, output, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]mem_20.0_model2\n",
      "mem data shape  -  20.0 ms\n",
      "train set : torch.Size([5000, 10, 200]) torch.Size([5000, 5])\n",
      "test set : torch.Size([1500, 10, 200]) torch.Size([1500, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/khj/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0.]\n",
      "epoch 0 - train loss : 0.9479183  /  test loss : 1.0009102\n",
      "           train acc : 0.3262000  /  test acc : 0.2000000\n",
      "[0.06333333 0.         0.98666667 0.         0.9       ]\n",
      "epoch 10 - train loss : 0.3753315  /  test loss : 1.2699715\n",
      "           train acc : 0.8388000  /  test acc : 0.3900000\n",
      "[0.98666667 0.68666667 1.         0.53333333 0.97333333]\n",
      "epoch 20 - train loss : 0.1459477  /  test loss : 0.2690511\n",
      "           train acc : 0.9488000  /  test acc : 0.8360000\n",
      "[0.98666667 0.64666667 1.         0.70666667 0.96      ]\n",
      "epoch 30 - train loss : 0.0554767  /  test loss : 0.2509457\n",
      "           train acc : 0.9892000  /  test acc : 0.8600000\n",
      "[0.99333333 0.64333333 1.         0.69       0.96      ]\n",
      "epoch 40 - train loss : 0.0216155  /  test loss : 0.2980193\n",
      "           train acc : 0.9984000  /  test acc : 0.8573333\n",
      "[0.99333333 0.67333333 1.         0.65666667 0.96333333]\n",
      "epoch 50 - train loss : 0.0102610  /  test loss : 0.3398875\n",
      "           train acc : 0.9998000  /  test acc : 0.8573333\n",
      "[0.99333333 0.66333333 1.         0.66333333 0.96666667]\n",
      "epoch 60 - train loss : 0.0063583  /  test loss : 0.3677871\n",
      "           train acc : 0.9998000  /  test acc : 0.8573333\n",
      "[0.99333333 0.65333333 1.         0.67666667 0.96666667]\n",
      "epoch 70 - train loss : 0.0047550  /  test loss : 0.3895886\n",
      "           train acc : 1.0000000  /  test acc : 0.8580000\n",
      "[0.99333333 0.66666667 1.         0.67666667 0.97      ]\n",
      "epoch 76 - train loss : 0.0036466  /  test loss : 0.3978291\n",
      "           train acc : 1.0000000  /  test acc : 0.8613333\n",
      "training complete! - calculation time : 10.765465021133423   seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    print('[Training]%s_%.1f_model%d'%(data_type, time_len, model_num))\n",
    "    torch.manual_seed(37)\n",
    "    torch.cuda.manual_seed_all(37)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    trainX = np.load('npy_data/%s/%s_%.1fms_trainX.npy'%(data_type, data_type, time_len))\n",
    "    trainY = np.load('npy_data/%s/%s_%.1fms_trainY.npy'%(data_type, data_type, time_len))\n",
    "    testX = np.load('npy_data/%s/%s_%.1fms_testX.npy'%(data_type, data_type, time_len))\n",
    "    testY = np.load('npy_data/%s/%s_%.1fms_testY.npy'%(data_type, data_type, time_len))\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    batch_num = 1000\n",
    "    batch = [int(len(trainX)/batch_num), batch_num]\n",
    "    \n",
    "    trainX = torch.Tensor(trainX).to(device)\n",
    "#     _, trainY = torch.max(torch.LongTensor(trainY).to(device), 1)\n",
    "    trainY = torch.Tensor(trainY).to(device)\n",
    "    testX = torch.Tensor(testX).to(device)\n",
    "    testY = torch.Tensor(testY).to(device)\n",
    "#     _, testY = torch.max(torch.LongTensor(testY).to(device), 1)\n",
    "    print('mem data shape  -  %.1f ms'%time_len)\n",
    "    print('train set :', np.shape(trainX) , np.shape(trainY))\n",
    "    print('test set :', np.shape(testX) ,np.shape(testY))\n",
    "    \n",
    "    \n",
    "    learning_rate = 0.00005\n",
    "    loss_func=nn.BCELoss()\n",
    "    model = nn.DataParallel(Model(loss_func)).to(device)\n",
    "#     model = Model(loss_func).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=batch_num, eta_min = 3e-6)\n",
    "\n",
    "#     model.load_state_dict(torch.load('ckpt/model%d/%s/%.1f_ckpt_100.pt'%(model_num, data_type, time_len)))\n",
    "    a = time.time()\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    for epoch in range(77):\n",
    "        train_loss, train_acc = train(model, trainX, trainY, batch, device, optimizer, train_loss, train_acc)\n",
    "        test_loss, test_acc, output, target = test(model, testX, testY, device, test_loss, test_acc)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch%10==0:\n",
    "            vowel_check = (output==target).cpu().detach().numpy()\n",
    "            vowel_acc = np.zeros(5)\n",
    "            for k1 in range(15):\n",
    "                for k2 in range(5):\n",
    "                    vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "            print(vowel_acc/(vow_num*15))\n",
    "            print('epoch %d - train loss : %.7f  /  test loss : %.7f'%(epoch, train_loss[-1], test_loss[-1]))\n",
    "            print('           train acc : %.7f  /  test acc : %.7f'%(train_acc[-1], test_acc[-1]))\n",
    "#         if epoch%50==0:\n",
    "#             print('@@@@@@@ save model : epoch %d'% epoch)\n",
    "#             torch.save(model.state_dict(),'ckpt/model%d_%s/%.1f_ckpt_%d.pt'%(model_num, data_type, time_len, epoch))\n",
    "#             np.savetxt('result/model%d_%s/%.1f_loss_tr.txt'%(model_num, data_type, time_len), train_loss)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_loss_te.txt'%(model_num, data_type, time_len), test_loss)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_acc_tr.txt'%(model_num, data_type, time_len), train_acc)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_acc_te.txt'%(model_num, data_type, time_len), test_acc)\n",
    "\n",
    "    vowel_check = (output==target).cpu().detach().numpy()\n",
    "    vowel_acc = np.zeros(5)\n",
    "    for k1 in range(15):\n",
    "        for k2 in range(5):\n",
    "            vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "    print(vowel_acc/(vow_num*15))\n",
    "    print('epoch %d - train loss : %.7f  /  test loss : %.7f'%(epoch, train_loss[-1], test_loss[-1]))\n",
    "    print('           train acc : %.7f  /  test acc : %.7f'%(train_acc[-1], test_acc[-1]))\n",
    "    print(\"training complete! - calculation time :\", time.time()-a, '  seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.846\n",
      "0.8066666666666666\n",
      "0.8266666666666667\n",
      "0.8613333333333333\n",
      "677\n",
      "785\n",
      "269\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "print(np.max(np.loadtxt('result/model2/mfcc/20.0_acc_te.txt')))\n",
    "print(np.max(np.loadtxt('result/model2/raw/20.0_acc_te.txt')))\n",
    "print(np.max(np.loadtxt('result/model2/fft/20.0_acc_te.txt')))\n",
    "print(np.max(np.loadtxt('result/model2/mem/20.0_acc_te.txt')))\n",
    "print(np.argmax(np.loadtxt('result/model2/mfcc/20.0_acc_te.txt')))\n",
    "print(np.argmax(np.loadtxt('result/model2/raw/20.0_acc_te.txt')))\n",
    "print(np.argmax(np.loadtxt('result/model2/fft/20.0_acc_te.txt')))\n",
    "print(np.argmax(np.loadtxt('result/model2/mem/20.0_acc_te.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole accuracy : [0.99333333 0.66666667 1.         0.67666667 0.97      ]\n",
      "[4 4 4 4 4 4 2 4 4 4 4 4 4 4 2 4 4 4 4 4]\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 4 4 4 2]\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[4 2 2 2 4 4 2 2 2 4 4 4 2 4 2 4 2 2 4 2]\n",
      "[4 4 4 4 2 2 2 4 4 4 2 2 4 2 2 4 4 2 4 2]\n",
      "[4 4 4 4 2 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[2 2 2 2 2 2 2 2 2 4 4 2 2 4 2 2 2 4 2 4]\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[4 4 4 4 4 4 2 4 2 2 4 4 4 4 4 4 4 4 4 4]\n",
      "[4 4 4 4 2 4 4 4 4 2 4 4 2 4 2 4 4 2 2 4]\n",
      "[4 2 2 2 2 4 2 4 4 2 2 2 2 2 2 2 2 2 4 2]\n",
      "[4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "part accuracy \n",
      " {0.0: 0.6766666666666666, 1.0: 0.06333333333333334, 2.0: 0.26}\n"
     ]
    }
   ],
   "source": [
    "vowel_check = (output==target).cpu().detach().numpy()\n",
    "vowel_acc = np.zeros(5)\n",
    "for k1 in range(15):\n",
    "    for k2 in range(5):\n",
    "        vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "print('whole accuracy :', vowel_acc/(vow_num*15))\n",
    "            \n",
    "vowel_check = (output==target).cpu().detach().numpy()\n",
    "whole_count=np.array([])\n",
    "for k1 in range(15):\n",
    "    for k2 in range(5):\n",
    "        if k2==3:\n",
    "            aa = output[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)].cpu().detach().numpy()+1\n",
    "            print(aa)\n",
    "            bb = aa*((-1*np.int32(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)]))+1)\n",
    "            whole_count = np.append(whole_count,bb)\n",
    "unique, counts = np.unique(whole_count, return_counts=True)\n",
    "#         vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "print('\\npart accuracy \\n', dict(zip(unique, counts/(len(output)/5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
