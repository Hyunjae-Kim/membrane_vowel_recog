{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'mfcc'\n",
    "model_num = 2\n",
    "time_len =  20  ## ms\n",
    "if time_len==0.5:\n",
    "    point_len = 25\n",
    "else:\n",
    "    point_len = int(45000*time_len/1000)\n",
    "div_num = int(450*(200/point_len))\n",
    "vow_num = int(div_num/5)\n",
    "\n",
    "point_len = 20\n",
    "fc_len = int(point_len/32)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, loss):\n",
    "        super(Model, self).__init__()\n",
    "        prob = 0.8\n",
    "        input_c = 1\n",
    "        channel = 32\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_c, channel, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(channel, channel*2, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*2))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(channel*2, channel*2, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*2))\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(channel*2, channel*4, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*4))\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(channel*4, channel*4, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*4))\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(channel*4*fc_len, 1024),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(1024))\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 5),\n",
    "            nn.Softmax(dim=1))\n",
    "        self.loss = loss\n",
    "        \n",
    "    def forward(self, data, target):\n",
    "        x = self.conv1(data)\n",
    "#         print('1',x.size())\n",
    "        x = self.conv2(x)\n",
    "#         print('2',x.size())\n",
    "        x = self.conv3(x)\n",
    "#         print('3',x.size())\n",
    "        x = self.conv4(x)\n",
    "#         print('4',x.size())\n",
    "        x = self.conv5(x)\n",
    "#         print('5',x.size())\n",
    "        x = x.view(x.size()[0],-1)\n",
    "        x = self.fc1(x)\n",
    "#         print('8',x.size())\n",
    "        h = self.fc2(x)\n",
    "#         print('9',h.size())\n",
    "        \n",
    "        l = self.loss(h, target)\n",
    "        return l, h, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainX, trainY, batch, device, optimizer, train_loss, train_acc):\n",
    "    model.train()\n",
    "    dloss = 0\n",
    "    dacc = 0\n",
    "    \n",
    "    rand = torch.randperm(trainX.size()[0])\n",
    "    trainX = trainX[rand]\n",
    "    trainY = trainY[rand]\n",
    "    \n",
    "    for i in range(batch[0]):\n",
    "        optimizer.zero_grad()\n",
    "        loss, output, target = model(trainX[i*batch[1]:(i+1)*batch[1]], trainY[i*batch[1]:(i+1)*batch[1]])\n",
    "        loss = loss.sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, output = torch.max(output, 1)\n",
    "        _, target = torch.max(target, 1)\n",
    "\n",
    "        dloss += loss.cpu().item()\n",
    "        dacc += (output==target).sum().item()\n",
    "        \n",
    "    train_loss.append(dloss/batch[0])\n",
    "    train_acc.append(dacc/(batch[0]*batch[1]))\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test(model, testX, testY, device, test_loss, test_acc):\n",
    "    model.eval()\n",
    "    loss, output, target = model(testX, testY)\n",
    "    loss = loss.sum()\n",
    "    \n",
    "    _, output = torch.max(output, 1)\n",
    "    _, target = torch.max(target, 1)\n",
    "    \n",
    "    test_loss.append(loss.cpu().item())\n",
    "    test_acc.append((output==target).sum().item()/len(testX))\n",
    "    return test_loss, test_acc, output, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]mfcc_20.0_model2\n",
      "mfcc data shape  -  20.0 ms\n",
      "train set : torch.Size([5000, 1, 20]) torch.Size([5000, 5])\n",
      "test set : torch.Size([1500, 1, 20]) torch.Size([1500, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/khj/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0.]\n",
      "epoch 0 - train loss : 0.9501075  /  test loss : 1.0007136\n",
      "           train acc : 0.3576000  /  test acc : 0.2000000\n",
      "[0.97666667 0.77666667 0.93333333 0.81       0.57      ]\n",
      "epoch 10 - train loss : 0.3370210  /  test loss : 0.4412613\n",
      "           train acc : 0.8708000  /  test acc : 0.8133333\n",
      "[0.97       0.84       0.89666667 0.78333333 0.67333333]\n",
      "epoch 20 - train loss : 0.1813561  /  test loss : 0.3188760\n",
      "           train acc : 0.9302000  /  test acc : 0.8326667\n",
      "[0.98       0.85666667 0.88       0.74333333 0.72666667]\n",
      "epoch 30 - train loss : 0.1086026  /  test loss : 0.3030239\n",
      "           train acc : 0.9646000  /  test acc : 0.8373333\n",
      "[0.98666667 0.86       0.88       0.73       0.75333333]\n",
      "epoch 40 - train loss : 0.0661972  /  test loss : 0.3075047\n",
      "           train acc : 0.9844000  /  test acc : 0.8420000\n",
      "[0.98666667 0.87       0.88       0.71333333 0.74333333]\n",
      "epoch 50 - train loss : 0.0423266  /  test loss : 0.3270770\n",
      "           train acc : 0.9924000  /  test acc : 0.8386667\n",
      "[0.99       0.86666667 0.89       0.74666667 0.70333333]\n",
      "epoch 60 - train loss : 0.0269536  /  test loss : 0.3390669\n",
      "           train acc : 0.9962000  /  test acc : 0.8393333\n",
      "[0.99       0.86333333 0.91       0.71666667 0.71333333]\n",
      "epoch 70 - train loss : 0.0170882  /  test loss : 0.3640791\n",
      "           train acc : 0.9984000  /  test acc : 0.8386667\n",
      "[0.99       0.86333333 0.91       0.74       0.71      ]\n",
      "epoch 80 - train loss : 0.0120617  /  test loss : 0.3801181\n",
      "           train acc : 0.9992000  /  test acc : 0.8426667\n",
      "[0.99       0.86333333 0.91333333 0.73666667 0.70666667]\n",
      "epoch 90 - train loss : 0.0089629  /  test loss : 0.3918253\n",
      "           train acc : 0.9998000  /  test acc : 0.8420000\n",
      "[0.99       0.86333333 0.91333333 0.74       0.7       ]\n",
      "epoch 100 - train loss : 0.0065274  /  test loss : 0.4097697\n",
      "           train acc : 1.0000000  /  test acc : 0.8413333\n",
      "[0.99       0.86       0.91333333 0.74       0.70333333]\n",
      "epoch 110 - train loss : 0.0050163  /  test loss : 0.4242777\n",
      "           train acc : 1.0000000  /  test acc : 0.8413333\n",
      "[0.99       0.86       0.91333333 0.74       0.7       ]\n",
      "epoch 120 - train loss : 0.0045410  /  test loss : 0.4359301\n",
      "           train acc : 1.0000000  /  test acc : 0.8406667\n",
      "[0.99       0.85666667 0.91333333 0.74333333 0.69      ]\n",
      "epoch 130 - train loss : 0.0032775  /  test loss : 0.4404578\n",
      "           train acc : 1.0000000  /  test acc : 0.8386667\n",
      "[0.98666667 0.85666667 0.91       0.74666667 0.69333333]\n",
      "epoch 140 - train loss : 0.0029017  /  test loss : 0.4469492\n",
      "           train acc : 1.0000000  /  test acc : 0.8386667\n",
      "[0.98666667 0.85666667 0.91       0.74333333 0.69333333]\n",
      "epoch 150 - train loss : 0.0024632  /  test loss : 0.4550456\n",
      "           train acc : 1.0000000  /  test acc : 0.8380000\n",
      "[0.98666667 0.85666667 0.91333333 0.75       0.68      ]\n",
      "epoch 160 - train loss : 0.0020325  /  test loss : 0.4601085\n",
      "           train acc : 1.0000000  /  test acc : 0.8373333\n",
      "[0.98666667 0.86       0.91333333 0.74333333 0.68      ]\n",
      "epoch 170 - train loss : 0.0018087  /  test loss : 0.4648969\n",
      "           train acc : 1.0000000  /  test acc : 0.8366667\n",
      "[0.98666667 0.85666667 0.91333333 0.75       0.68      ]\n",
      "epoch 180 - train loss : 0.0016900  /  test loss : 0.4721767\n",
      "           train acc : 1.0000000  /  test acc : 0.8373333\n",
      "[0.98666667 0.85666667 0.91333333 0.75       0.68      ]\n",
      "epoch 190 - train loss : 0.0014261  /  test loss : 0.4775797\n",
      "           train acc : 1.0000000  /  test acc : 0.8373333\n",
      "[0.99       0.85333333 0.91333333 0.75333333 0.68      ]\n",
      "epoch 200 - train loss : 0.0013455  /  test loss : 0.4798917\n",
      "           train acc : 1.0000000  /  test acc : 0.8380000\n",
      "[0.98666667 0.86666667 0.91333333 0.74666667 0.68      ]\n",
      "epoch 210 - train loss : 0.0012080  /  test loss : 0.4876373\n",
      "           train acc : 1.0000000  /  test acc : 0.8386667\n",
      "[0.98666667 0.86666667 0.91333333 0.75333333 0.68      ]\n",
      "epoch 220 - train loss : 0.0011439  /  test loss : 0.4866457\n",
      "           train acc : 1.0000000  /  test acc : 0.8400000\n",
      "[0.98666667 0.86666667 0.91666667 0.74333333 0.68666667]\n",
      "epoch 230 - train loss : 0.0009974  /  test loss : 0.4924573\n",
      "           train acc : 1.0000000  /  test acc : 0.8400000\n",
      "[0.99       0.86333333 0.92       0.75333333 0.68      ]\n",
      "epoch 240 - train loss : 0.0009251  /  test loss : 0.4910757\n",
      "           train acc : 1.0000000  /  test acc : 0.8413333\n",
      "[0.98666667 0.86333333 0.91333333 0.76       0.68333333]\n",
      "epoch 250 - train loss : 0.0009747  /  test loss : 0.4968572\n",
      "           train acc : 1.0000000  /  test acc : 0.8413333\n",
      "[0.98666667 0.86666667 0.91333333 0.75333333 0.67666667]\n",
      "epoch 260 - train loss : 0.0008580  /  test loss : 0.5054526\n",
      "           train acc : 1.0000000  /  test acc : 0.8393333\n",
      "[0.98666667 0.86333333 0.91666667 0.75666667 0.68      ]\n",
      "epoch 270 - train loss : 0.0007500  /  test loss : 0.5037925\n",
      "           train acc : 1.0000000  /  test acc : 0.8406667\n",
      "[0.98666667 0.86333333 0.91666667 0.75666667 0.68      ]\n",
      "epoch 280 - train loss : 0.0006893  /  test loss : 0.5082453\n",
      "           train acc : 1.0000000  /  test acc : 0.8406667\n",
      "[0.99       0.85666667 0.91666667 0.76       0.67666667]\n",
      "epoch 290 - train loss : 0.0006539  /  test loss : 0.5093998\n",
      "           train acc : 1.0000000  /  test acc : 0.8400000\n",
      "[0.99       0.85666667 0.91666667 0.75333333 0.68      ]\n",
      "epoch 300 - train loss : 0.0005841  /  test loss : 0.5134573\n",
      "           train acc : 1.0000000  /  test acc : 0.8393333\n",
      "[0.98666667 0.86333333 0.91666667 0.76       0.67666667]\n",
      "epoch 310 - train loss : 0.0005699  /  test loss : 0.5129842\n",
      "           train acc : 1.0000000  /  test acc : 0.8406667\n",
      "[0.99       0.85666667 0.91666667 0.76       0.67333333]\n",
      "epoch 320 - train loss : 0.0005527  /  test loss : 0.5170672\n",
      "           train acc : 1.0000000  /  test acc : 0.8393333\n",
      "[0.99       0.87       0.91666667 0.76       0.67666667]\n",
      "epoch 330 - train loss : 0.0004921  /  test loss : 0.5250988\n",
      "           train acc : 1.0000000  /  test acc : 0.8426667\n",
      "[0.99       0.86666667 0.91666667 0.75666667 0.67666667]\n",
      "epoch 340 - train loss : 0.0005037  /  test loss : 0.5252941\n",
      "           train acc : 1.0000000  /  test acc : 0.8413333\n",
      "[0.98666667 0.85666667 0.91666667 0.75666667 0.68      ]\n",
      "epoch 350 - train loss : 0.0004604  /  test loss : 0.5292555\n",
      "           train acc : 1.0000000  /  test acc : 0.8393333\n",
      "[0.99       0.85666667 0.92       0.76666667 0.67333333]\n",
      "epoch 360 - train loss : 0.0004280  /  test loss : 0.5308365\n",
      "           train acc : 1.0000000  /  test acc : 0.8413333\n",
      "[0.99       0.85666667 0.91666667 0.76       0.67333333]\n",
      "epoch 370 - train loss : 0.0004157  /  test loss : 0.5334434\n",
      "           train acc : 1.0000000  /  test acc : 0.8393333\n",
      "[0.99       0.86       0.91666667 0.76       0.67333333]\n",
      "epoch 380 - train loss : 0.0003956  /  test loss : 0.5321268\n",
      "           train acc : 1.0000000  /  test acc : 0.8400000\n",
      "[0.99       0.86666667 0.91666667 0.75666667 0.67333333]\n",
      "epoch 390 - train loss : 0.0003848  /  test loss : 0.5344520\n",
      "           train acc : 1.0000000  /  test acc : 0.8406667\n",
      "[0.99       0.86333333 0.91666667 0.75666667 0.67333333]\n",
      "epoch 400 - train loss : 0.0004018  /  test loss : 0.5391135\n",
      "           train acc : 1.0000000  /  test acc : 0.8400000\n",
      "[0.99       0.85666667 0.92       0.76       0.67333333]\n",
      "epoch 410 - train loss : 0.0003531  /  test loss : 0.5389982\n",
      "           train acc : 1.0000000  /  test acc : 0.8400000\n",
      "[0.99       0.86       0.92       0.76333333 0.67      ]\n",
      "epoch 420 - train loss : 0.0003339  /  test loss : 0.5387255\n",
      "           train acc : 1.0000000  /  test acc : 0.8406667\n",
      "[0.99       0.86666667 0.91666667 0.76666667 0.67      ]\n",
      "epoch 430 - train loss : 0.0003052  /  test loss : 0.5425762\n",
      "           train acc : 1.0000000  /  test acc : 0.8420000\n",
      "[0.98666667 0.86333333 0.91666667 0.76       0.67      ]\n",
      "epoch 440 - train loss : 0.0003037  /  test loss : 0.5494484\n",
      "           train acc : 1.0000000  /  test acc : 0.8393333\n",
      "[0.99       0.86333333 0.91666667 0.76666667 0.67333333]\n",
      "epoch 450 - train loss : 0.0003035  /  test loss : 0.5470241\n",
      "           train acc : 1.0000000  /  test acc : 0.8420000\n",
      "[0.99       0.86333333 0.91666667 0.76       0.67333333]\n",
      "epoch 460 - train loss : 0.0002850  /  test loss : 0.5482659\n",
      "           train acc : 1.0000000  /  test acc : 0.8406667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99       0.86333333 0.92       0.77       0.66666667]\n",
      "epoch 470 - train loss : 0.0002853  /  test loss : 0.5513160\n",
      "           train acc : 1.0000000  /  test acc : 0.8420000\n",
      "[0.99       0.86333333 0.91666667 0.77333333 0.67      ]\n",
      "epoch 480 - train loss : 0.0002681  /  test loss : 0.5484827\n",
      "           train acc : 1.0000000  /  test acc : 0.8426667\n",
      "[0.99       0.86333333 0.91666667 0.75666667 0.67333333]\n",
      "epoch 490 - train loss : 0.0002613  /  test loss : 0.5524859\n",
      "           train acc : 1.0000000  /  test acc : 0.8400000\n",
      "[0.98666667 0.86666667 0.91666667 0.76666667 0.66666667]\n",
      "epoch 500 - train loss : 0.0002472  /  test loss : 0.5539078\n",
      "           train acc : 1.0000000  /  test acc : 0.8406667\n",
      "[0.99       0.86333333 0.91666667 0.77666667 0.67      ]\n",
      "epoch 510 - train loss : 0.0002429  /  test loss : 0.5534578\n",
      "           train acc : 1.0000000  /  test acc : 0.8433333\n",
      "[0.99       0.86333333 0.91666667 0.77       0.67666667]\n",
      "epoch 520 - train loss : 0.0002258  /  test loss : 0.5544816\n",
      "           train acc : 1.0000000  /  test acc : 0.8433333\n",
      "[0.99       0.86666667 0.91666667 0.77       0.67      ]\n",
      "epoch 530 - train loss : 0.0002223  /  test loss : 0.5562105\n",
      "           train acc : 1.0000000  /  test acc : 0.8426667\n",
      "[0.99       0.86333333 0.91666667 0.77666667 0.66      ]\n",
      "epoch 540 - train loss : 0.0002306  /  test loss : 0.5576324\n",
      "           train acc : 1.0000000  /  test acc : 0.8413333\n",
      "[0.99       0.86333333 0.92333333 0.76666667 0.66666667]\n",
      "epoch 550 - train loss : 0.0002040  /  test loss : 0.5638418\n",
      "           train acc : 1.0000000  /  test acc : 0.8420000\n",
      "[0.99       0.86666667 0.92       0.77       0.66333333]\n",
      "epoch 560 - train loss : 0.0002184  /  test loss : 0.5655405\n",
      "           train acc : 1.0000000  /  test acc : 0.8420000\n",
      "[0.99       0.86333333 0.92333333 0.77       0.67      ]\n",
      "epoch 570 - train loss : 0.0002011  /  test loss : 0.5635026\n",
      "           train acc : 1.0000000  /  test acc : 0.8433333\n",
      "[0.99       0.86666667 0.92       0.76666667 0.66666667]\n",
      "epoch 580 - train loss : 0.0002051  /  test loss : 0.5672916\n",
      "           train acc : 1.0000000  /  test acc : 0.8420000\n",
      "[0.99       0.86333333 0.92       0.77666667 0.66666667]\n",
      "epoch 590 - train loss : 0.0002034  /  test loss : 0.5644798\n",
      "           train acc : 1.0000000  /  test acc : 0.8433333\n",
      "[0.99       0.86666667 0.92       0.76666667 0.66666667]\n",
      "epoch 600 - train loss : 0.0002046  /  test loss : 0.5707426\n",
      "           train acc : 1.0000000  /  test acc : 0.8420000\n",
      "[0.99       0.86333333 0.91666667 0.77333333 0.67      ]\n",
      "epoch 610 - train loss : 0.0001868  /  test loss : 0.5670099\n",
      "           train acc : 1.0000000  /  test acc : 0.8426667\n",
      "[0.99       0.85666667 0.92       0.76666667 0.67      ]\n",
      "epoch 620 - train loss : 0.0001842  /  test loss : 0.5698398\n",
      "           train acc : 1.0000000  /  test acc : 0.8406667\n",
      "[0.99       0.86666667 0.91666667 0.76666667 0.66666667]\n",
      "epoch 630 - train loss : 0.0001708  /  test loss : 0.5698106\n",
      "           train acc : 1.0000000  /  test acc : 0.8413333\n",
      "[0.99       0.86333333 0.91666667 0.77333333 0.67      ]\n",
      "epoch 640 - train loss : 0.0001721  /  test loss : 0.5694881\n",
      "           train acc : 1.0000000  /  test acc : 0.8426667\n",
      "[0.99       0.86333333 0.91666667 0.77666667 0.67333333]\n",
      "epoch 650 - train loss : 0.0001693  /  test loss : 0.5725527\n",
      "           train acc : 1.0000000  /  test acc : 0.8440000\n",
      "[0.99       0.86333333 0.92       0.77333333 0.67      ]\n",
      "epoch 660 - train loss : 0.0001788  /  test loss : 0.5702380\n",
      "           train acc : 1.0000000  /  test acc : 0.8433333\n",
      "[0.99       0.86333333 0.92       0.77666667 0.66666667]\n",
      "epoch 670 - train loss : 0.0001787  /  test loss : 0.5755084\n",
      "           train acc : 1.0000000  /  test acc : 0.8433333\n",
      "training complete! - calculation time : 156.17651677131653   seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':    \n",
    "    print('[Training]%s_%.1f_model%d'%(data_type, time_len, model_num))\n",
    "    torch.manual_seed(37)\n",
    "    torch.cuda.manual_seed_all(37)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    trainX = np.load('npy_data/%s/%s_%.1fms_trainX.npy'%(data_type, data_type, time_len)).reshape(-1, 1, point_len)\n",
    "    trainY = np.load('npy_data/%s/%s_%.1fms_trainY.npy'%(data_type, data_type, time_len))\n",
    "    testX = np.load('npy_data/%s/%s_%.1fms_testX.npy'%(data_type, data_type, time_len)).reshape(-1, 1, point_len)\n",
    "    testY = np.load('npy_data/%s/%s_%.1fms_testY.npy'%(data_type, data_type, time_len))\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    batch_num = 1000\n",
    "    batch = [int(len(trainX)/batch_num), batch_num]\n",
    "    \n",
    "    trainX = torch.Tensor(trainX).to(device)\n",
    "    trainY = torch.Tensor(trainY).to(device)\n",
    "    testX = torch.Tensor(testX).to(device)\n",
    "    testY = torch.Tensor(testY).to(device)\n",
    "    print('%s data shape  -  %.1f ms'%(data_type, time_len))\n",
    "    print('train set :', np.shape(trainX) , np.shape(trainY))\n",
    "    print('test set :', np.shape(testX) ,np.shape(testY))\n",
    "    \n",
    "    \n",
    "    learning_rate = 0.00005\n",
    "    loss_func=nn.BCELoss()\n",
    "    model = nn.DataParallel(Model(loss_func)).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=batch_num, eta_min = 3e-6)\n",
    "\n",
    "#     model.load_state_dict(torch.load('ckpt/model%d/%s/%.1f_ckpt_1000.pt'%(model_num, data_type, time_len)))\n",
    "    a = time.time()\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    for epoch in range(678):\n",
    "        train_loss, train_acc = train(model, trainX, trainY, batch, device, optimizer, train_loss, train_acc)\n",
    "        test_loss, test_acc, output, target = test(model, testX, testY, device, test_loss, test_acc)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch%10==0: \n",
    "            vowel_check = (output==target).cpu().detach().numpy()\n",
    "            vowel_acc = np.zeros(5)\n",
    "            for k1 in range(15):\n",
    "                for k2 in range(5):\n",
    "                    vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "            print(vowel_acc/(vow_num*15))\n",
    "            print('epoch %d - train loss : %.7f  /  test loss : %.7f'%(epoch, train_loss[-1], test_loss[-1]))\n",
    "            print('           train acc : %.7f  /  test acc : %.7f'%(train_acc[-1], test_acc[-1]))\n",
    "#         if epoch%50==0:\n",
    "#             print('@@@@@@@ save model : epoch %d'% epoch)\n",
    "#             torch.save(model.state_dict(),'ckpt/model%d_%s/%.1f_ckpt_%d.pt'%(model_num, data_type, time_len, epoch))\n",
    "#             np.savetxt('result/model%d_%s/%.1f_loss_tr.txt'%(model_num, data_type, time_len), train_loss)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_loss_te.txt'%(model_num, data_type, time_len), test_loss)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_acc_tr.txt'%(model_num, data_type, time_len), train_acc)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_acc_te.txt'%(model_num, data_type, time_len), test_acc)\n",
    "    print(\"training complete! - calculation time :\", time.time()-a, '  seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole accuracy : [0.99       0.86333333 0.92666667 0.77666667 0.67333333]\n",
      "\n",
      "part accuracy \n",
      " {0.0: 0.6733333333333333, 2.0: 0.06666666666666667, 3.0: 0.06666666666666667, 4.0: 0.19333333333333333}\n"
     ]
    }
   ],
   "source": [
    "vowel_check = (output==target).cpu().detach().numpy()\n",
    "vowel_acc = np.zeros(5)\n",
    "for k1 in range(15):\n",
    "    for k2 in range(5):\n",
    "        vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "print('whole accuracy :', vowel_acc/(vow_num*15))\n",
    "            \n",
    "vowel_check = (output==target).cpu().detach().numpy()\n",
    "whole_count=np.array([])\n",
    "for k1 in range(15):\n",
    "    for k2 in range(5):\n",
    "        if k2==4:\n",
    "            aa = output[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)].cpu().detach().numpy()+1\n",
    "#             print(aa)\n",
    "            bb = aa*((-1*np.int32(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)]))+1)\n",
    "            whole_count = np.append(whole_count,bb)\n",
    "unique, counts = np.unique(whole_count, return_counts=True)\n",
    "#         vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "print('\\npart accuracy \\n', dict(zip(unique, counts/(len(output)/5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
