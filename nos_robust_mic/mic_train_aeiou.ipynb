{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'mic_wide'\n",
    "time_len =  20  ## ms\n",
    "point_len = int(time_len*10)\n",
    "div_num = int(100*(20/time_len))\n",
    "vow_num = int(div_num/5)\n",
    "\n",
    "fc_len = int(point_len/32)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, loss):\n",
    "        super(Model, self).__init__()\n",
    "        input_c = 30\n",
    "        channel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_c, channel, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(channel, channel*2, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*2))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(channel*2, channel*2, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*2))\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(channel*2, channel*4, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*4))\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(channel*4, channel*4, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*4))\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(channel*4*fc_len, 1024),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(1024))\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 5),\n",
    "            nn.Softmax(dim=1))\n",
    "        self.loss = loss\n",
    "        \n",
    "    def forward(self, data, target):\n",
    "        x = self.conv1(data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = x.view(x.size()[0],-1)\n",
    "        x = self.fc1(x)\n",
    "        h = self.fc2(x)\n",
    "        \n",
    "        l = self.loss(h, target)\n",
    "        return l, h, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mod_tr, trX, trY, bat_tr, dev, opt_tr, tr_loss, tr_acc):\n",
    "    mod_tr.train()\n",
    "    dloss_tr = 0\n",
    "    dacc_tr = 0\n",
    "    \n",
    "    rand = torch.randperm(trX.size()[0])\n",
    "    trX = trX[rand]\n",
    "    trY = trY[rand]\n",
    "    \n",
    "    for i in range(bat_tr[0]):\n",
    "        opt_tr.zero_grad()\n",
    "        loss_tr, output_tr, target_tr = mod_tr(trX[i*bat_tr[1]:(i+1)*bat_tr[1]], \n",
    "                                            trY[i*bat_tr[1]:(i+1)*bat_tr[1]])\n",
    "        loss_tr = loss_tr.sum()\n",
    "        loss_tr.backward()\n",
    "        opt_tr.step()\n",
    "        \n",
    "        _, output_tr = torch.max(output_tr, 1)\n",
    "        _, target_tr = torch.max(target_tr, 1)\n",
    "\n",
    "        dloss_tr += loss_tr.cpu().item()\n",
    "        dacc_tr += (output_tr==target_tr).sum().item()\n",
    "\n",
    "    tr_loss.append(dloss_tr/bat_tr[0])\n",
    "    tr_acc.append(dacc_tr/(bat_tr[0]*bat_tr[1]))\n",
    "    return tr_loss, tr_acc\n",
    "\n",
    "def test(mod_te, teX, teY, bat_te, dev, te_loss, te_acc):\n",
    "    mod_te.eval()\n",
    "    dloss_te = 0\n",
    "    dacc_te = 0\n",
    "    \n",
    "    for i in range(bat_te[0]):\n",
    "        loss_te, output_te, target_te = mod_te(teX[i*bat_te[1]:(i+1)*bat_te[1]], \n",
    "                                              teY[i*bat_te[1]:(i+1)*bat_te[1]])\n",
    "        loss_te = loss_te.sum()\n",
    "        \n",
    "        _, output_te = torch.max(output_te, 1)\n",
    "        _, target_te = torch.max(target_te, 1)\n",
    "\n",
    "        dloss_te += loss_te.cpu().item()\n",
    "        dacc_te += (output_te==target_te).sum().item()\n",
    "        \n",
    "    te_loss.append(dloss_te/bat_te[0])\n",
    "    te_acc.append(dacc_te/(bat_te[0]*bat_te[1]))\n",
    "    return te_loss, te_acc, output_te, target_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]mic_wide_20.0\n",
      "mic_wide data shape  -  20.0 ms\n",
      "train set : torch.Size([5000, 30, 200]) torch.Size([5000, 5])\n",
      "test set : torch.Size([1500, 30, 200]) torch.Size([1500, 5])\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    print('[Training]%s_%.1f'%(data_type, time_len))\n",
    "    torch.manual_seed(37)\n",
    "    torch.cuda.manual_seed_all(37)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    trainX = np.load('npy_data/%s/%s_%.1fms_trainX.npy'%(data_type, data_type, time_len))\n",
    "    trainY = np.load('npy_data/%s/%s_%.1fms_trainY.npy'%(data_type, data_type, time_len))\n",
    "    testX = np.load('npy_data/%s/%s_%.1fms_testX.npy'%(data_type, data_type, time_len))\n",
    "    testY = np.load('npy_data/%s/%s_%.1fms_testY.npy'%(data_type, data_type, time_len))\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    batch_num = int(len(testX)/3)\n",
    "    batch_tr = [int(len(trainX)/batch_num), batch_num]\n",
    "    batch_te = [int(len(testX)), batch_num*3]\n",
    "    \n",
    "    trainX = torch.Tensor(trainX).to(device)\n",
    "    trainY = torch.Tensor(trainY).to(device)\n",
    "    testX = torch.Tensor(testX).to(device)\n",
    "    testY = torch.Tensor(testY).to(device)\n",
    "    print('%s data shape  -  %.1f ms'%(data_type, time_len))\n",
    "    print('train set :', np.shape(trainX) , np.shape(trainY))\n",
    "    print('test set :', np.shape(testX) ,np.shape(testY))\n",
    "    \n",
    "    \n",
    "    learning_rate = 0.0005\n",
    "    loss_func=nn.BCELoss()\n",
    "    model = nn.DataParallel(Model(loss_func)).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=batch_num, eta_min = 3e-6)\n",
    "\n",
    "#     model.load_state_dict(torch.load('ckpt/model%d/%s/%.1f_ckpt_100.pt'%(model_num, data_type, time_len)))\n",
    "    a = time.time()\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    for epoch in range(1001):\n",
    "        train_loss, train_acc = train(model, trainX, trainY, batch_tr, device, \n",
    "                                      optimizer, train_loss, train_acc)\n",
    "        test_loss, test_acc, output, target = test(model, testX, testY, batch_te, device, \n",
    "                                   test_loss, test_acc)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch%10==0:\n",
    "            vowel_check = (output==target).cpu().detach().numpy()\n",
    "            vowel_acc = np.zeros(5)\n",
    "            for k1 in range(15):\n",
    "                for k2 in range(5):\n",
    "                    vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "            print(vowel_acc/(vow_num*15))\n",
    "            print('epoch %d - train loss : %.7f  /  test loss : %.7f'%(epoch, train_loss[-1], test_loss[-1]))\n",
    "            print('           train acc : %.7f  /  test acc : %.7f'%(train_acc[-1], test_acc[-1]))\n",
    "        if epoch%10==0:\n",
    "            print('@@@@@@@ save model : epoch %d'% epoch)\n",
    "            torch.save(model.state_dict(),'ckpt/%s/%.1f_ckpt_%d.pt'%(data_type, time_len, epoch))\n",
    "            np.savetxt('result/%s/%.1f_loss_tr.txt'%(data_type, time_len), train_loss)\n",
    "            np.savetxt('result/%s/%.1f_loss_te.txt'%(data_type, time_len), test_loss)\n",
    "            np.savetxt('result/%s/%.1f_acc_tr.txt'%(data_type, time_len), train_acc)\n",
    "            np.savetxt('result/%s/%.1f_acc_te.txt'%(data_type, time_len), test_acc)\n",
    "\n",
    "    vowel_check = (output==target).cpu().detach().numpy()\n",
    "    vowel_acc = np.zeros(5)\n",
    "    for k1 in range(15):\n",
    "        for k2 in range(5):\n",
    "            vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "    print(vowel_acc/(vow_num*15))\n",
    "    print('epoch %d - train loss : %.7f  /  test loss : %.7f'%(epoch, train_loss[-1], test_loss[-1]))\n",
    "    print('           train acc : %.7f  /  test acc : %.7f'%(train_acc[-1], test_acc[-1]))\n",
    "    print(\"training complete! - calculation time :\", time.time()-a, '  seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(np.loadtxt('result/mic_wide/20.0_acc_te.txt')))\n",
    "print(np.argmax(np.loadtxt('result/mic_wide/20.0_acc_te.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel_check = (output==target).cpu().detach().numpy()\n",
    "vowel_acc = np.zeros(5)\n",
    "for k1 in range(15):\n",
    "    for k2 in range(5):\n",
    "        vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "print('whole accuracy :', vowel_acc/(vow_num*15))\n",
    "            \n",
    "vowel_check = (output==target).cpu().detach().numpy()\n",
    "whole_count=np.array([])\n",
    "for k1 in range(15):\n",
    "    for k2 in range(5):\n",
    "        if k2==3:\n",
    "            aa = output[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)].cpu().detach().numpy()+1\n",
    "            print(aa)\n",
    "            bb = aa*((-1*np.int32(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)]))+1)\n",
    "            whole_count = np.append(whole_count,bb)\n",
    "unique, counts = np.unique(whole_count, return_counts=True)\n",
    "#         vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "print('\\npart accuracy \\n', dict(zip(unique, counts/(len(output)/5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
