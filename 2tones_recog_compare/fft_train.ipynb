{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'fft'\n",
    "model_num = 2\n",
    "time_len =  0.5  ## ms\n",
    "if time_len==0.5:\n",
    "    point_len = 25\n",
    "else:\n",
    "    point_len = int(45000*time_len/1000)\n",
    "div_num = int(450*(200/point_len))\n",
    "vow_num = int(div_num/5)\n",
    "    \n",
    "if time_len==0.5:\n",
    "    point_len = 13\n",
    "else:\n",
    "    point_len = int(45000*time_len/1000/2)+1\n",
    "\n",
    "fc_len = int(point_len/32)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, loss):\n",
    "        super(Model, self).__init__()\n",
    "        prob = 0.8\n",
    "        input_c = 1\n",
    "        channel = 32\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_c, channel, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(channel, channel*2, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*2))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(channel*2, channel*2, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*2))\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(channel*2, channel*4, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*4))\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(channel*4, channel*4, kernel_size=5, stride=2, padding=2),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*4))\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(channel*4*fc_len, 1024),\n",
    "#             nn.Dropout(p=prob),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(1024))\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 5),\n",
    "            nn.Softmax(dim=1))\n",
    "        self.loss = loss\n",
    "        \n",
    "    def forward(self, data, target):\n",
    "        x = self.conv1(data)\n",
    "#         print('1',x.size())\n",
    "        x = self.conv2(x)\n",
    "#         print('2',x.size())\n",
    "        x = self.conv3(x)\n",
    "#         print('3',x.size())\n",
    "        x = self.conv4(x)\n",
    "#         print('4',x.size())\n",
    "        x = self.conv5(x)\n",
    "        x = x.view(x.size()[0],-1)\n",
    "        x = self.fc1(x)\n",
    "#         print('8',x.size())\n",
    "        h = self.fc2(x)\n",
    "#         print('9',h.size())\n",
    "        \n",
    "        l = self.loss(h, target)\n",
    "        return l, h, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainX, trainY, batch, device, optimizer, train_loss, train_acc):\n",
    "    model.train()\n",
    "    dloss = 0\n",
    "    dacc = 0\n",
    "    \n",
    "    rand = torch.randperm(trainX.size()[0])\n",
    "    trainX = trainX[rand]\n",
    "    trainY = trainY[rand]\n",
    "    \n",
    "    for i in range(batch[0]):\n",
    "        optimizer.zero_grad()\n",
    "        loss, output, target = model(trainX[i*batch[1]:(i+1)*batch[1]], trainY[i*batch[1]:(i+1)*batch[1]])\n",
    "        loss = loss.sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, output = torch.max(output, 1)\n",
    "        _, target = torch.max(target, 1)\n",
    "\n",
    "        dloss += loss.cpu().item()\n",
    "        dacc += (output==target).sum().item()\n",
    "        \n",
    "    train_loss.append(dloss/batch[0])\n",
    "    train_acc.append(dacc/(batch[0]*batch[1]))\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test(model, testX, testY, device, test_loss, test_acc):\n",
    "    model.eval()\n",
    "    loss, output, target = model(testX, testY)\n",
    "    loss = loss.sum()\n",
    "    \n",
    "    _, output = torch.max(output, 1)\n",
    "    _, target = torch.max(target, 1)\n",
    "    \n",
    "    test_loss.append(loss.cpu().item())\n",
    "    test_acc.append((output==target).sum().item()/len(testX))\n",
    "    return test_loss, test_acc, output, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]fft_0.5_model2\n",
      "fft data shape  -  0.5 ms\n",
      "train set : torch.Size([180000, 1, 13]) torch.Size([180000, 5])\n",
      "test set : torch.Size([54000, 1, 13]) torch.Size([54000, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/khj/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.54203704 0.33574074 0.62814815 0.51685185 0.23611111]\n",
      "epoch 0 - train loss : 0.6281646  /  test loss : 0.9826522\n",
      "           train acc : 0.6145667  /  test acc : 0.4517778\n",
      "training complete! - calculation time : 3.5715761184692383   seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':    \n",
    "    print('[Training]%s_%.1f_model%d'%(data_type, time_len, model_num))\n",
    "    torch.manual_seed(37)\n",
    "    torch.cuda.manual_seed_all(37)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    trainX = np.load('npy_data/%s/%s_%.1fms_trainX.npy'%(data_type, data_type, time_len)).reshape(-1, 1, point_len)\n",
    "    trainY = np.load('npy_data/%s/%s_%.1fms_trainY.npy'%(data_type, data_type, time_len))\n",
    "    testX = np.load('npy_data/%s/%s_%.1fms_testX.npy'%(data_type, data_type, time_len)).reshape(-1, 1, point_len)\n",
    "    testY = np.load('npy_data/%s/%s_%.1fms_testY.npy'%(data_type, data_type, time_len))\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    batch_num = 1000\n",
    "    batch = [int(len(trainX)/batch_num), batch_num]\n",
    "    \n",
    "    trainX = torch.Tensor(trainX).to(device)\n",
    "    trainY = torch.Tensor(trainY).to(device)\n",
    "    testX = torch.Tensor(testX).to(device)\n",
    "    testY = torch.Tensor(testY).to(device)\n",
    "    print('%s data shape  -  %.1f ms'%(data_type, time_len))\n",
    "    print('train set :', np.shape(trainX) , np.shape(trainY))\n",
    "    print('test set :', np.shape(testX) ,np.shape(testY))\n",
    "    \n",
    "    \n",
    "    learning_rate = 0.00005\n",
    "    loss_func=nn.BCELoss()\n",
    "    model = nn.DataParallel(Model(loss_func)).to(device)\n",
    "#     model = Model(loss_func).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=batch_num, eta_min = 3e-6)\n",
    "\n",
    "    model.load_state_dict(torch.load('ckpt/model%d/%s/%.1f_ckpt_1000.pt'%(model_num, data_type, time_len)))\n",
    "    a = time.time()\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    for epoch in range(1):\n",
    "        train_loss, train_acc = train(model, trainX, trainY, batch, device, optimizer, train_loss, train_acc)\n",
    "        test_loss, test_acc, output, target = test(model, testX, testY, device, test_loss, test_acc)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch%10==0: \n",
    "            vowel_check = (output==target).cpu().detach().numpy()\n",
    "            vowel_acc = np.zeros(5)\n",
    "            for k1 in range(15):\n",
    "                for k2 in range(5):\n",
    "                    vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "            print(vowel_acc/(vow_num*15))\n",
    "            print('epoch %d - train loss : %.7f  /  test loss : %.7f'%(epoch, train_loss[-1], test_loss[-1]))\n",
    "            print('           train acc : %.7f  /  test acc : %.7f'%(train_acc[-1], test_acc[-1]))\n",
    "#         if epoch%50==0:\n",
    "#             print('@@@@@@@ save model : epoch %d'% epoch)\n",
    "#             torch.save(model.state_dict(),'ckpt/model%d_%s/%.1f_ckpt_%d.pt'%(model_num, data_type, time_len, epoch))\n",
    "#             np.savetxt('result/model%d_%s/%.1f_loss_tr.txt'%(model_num, data_type, time_len), train_loss)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_loss_te.txt'%(model_num, data_type, time_len), test_loss)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_acc_tr.txt'%(model_num, data_type, time_len), train_acc)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_acc_te.txt'%(model_num, data_type, time_len), test_acc)\n",
    "    print(\"training complete! - calculation time :\", time.time()-a, '  seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole accuracy : [0.54203704 0.33574074 0.62814815 0.51685185 0.23611111]\n",
      "\n",
      "part accuracy \n",
      " {0.0: 0.2361111111111111, 1.0: 0.0874074074074074, 2.0: 0.1212037037037037, 3.0: 0.24768518518518517, 4.0: 0.3075925925925926}\n"
     ]
    }
   ],
   "source": [
    "vowel_check = (output==target).cpu().detach().numpy()\n",
    "vowel_acc = np.zeros(5)\n",
    "for k1 in range(15):\n",
    "    for k2 in range(5):\n",
    "        vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "print('whole accuracy :', vowel_acc/(vow_num*15))\n",
    "            \n",
    "vowel_check = (output==target).cpu().detach().numpy()\n",
    "whole_count=np.array([])\n",
    "for k1 in range(15):\n",
    "    for k2 in range(5):\n",
    "        if k2==4:\n",
    "            aa = output[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)].cpu().detach().numpy()+1\n",
    "#             print(aa)\n",
    "            bb = aa*((-1*np.int32(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)]))+1)\n",
    "            whole_count = np.append(whole_count,bb)\n",
    "unique, counts = np.unique(whole_count, return_counts=True)\n",
    "#         vowel_acc[k2] += np.sum(vowel_check[div_num*k1 + vow_num*k2 : div_num*k1 + vow_num*(k2+1)])\n",
    "print('\\npart accuracy \\n', dict(zip(unique, counts/(len(output)/5))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
