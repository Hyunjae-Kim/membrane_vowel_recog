{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'mem'\n",
    "model_num = 2\n",
    "time_len =  0.5  ## ms\n",
    "\n",
    "point_len = int(np.ceil(time_len*5))\n",
    "fc_len = int(point_len/32)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, loss):\n",
    "        super(Model, self).__init__()\n",
    "        input_c = 6\n",
    "        channel = 32\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_c, channel, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(channel, channel*2, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*2))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(channel*2, channel*2, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*2))\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(channel*2, channel*4, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*4))\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(channel*4, channel*4, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(channel*4))\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(channel*4*fc_len, 1024),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.BatchNorm1d(1024))\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 20),\n",
    "            nn.Softmax(dim=1))\n",
    "        self.loss = loss\n",
    "        \n",
    "    def forward(self, data, target):\n",
    "        x = self.conv1(data)\n",
    "#         print('1',x.size())\n",
    "        x = self.conv2(x)\n",
    "#         print('2',x.size())\n",
    "        x = self.conv3(x)\n",
    "#         print('3',x.size())\n",
    "        x = self.conv4(x)\n",
    "#         print('4',x.size())\n",
    "        x = self.conv5(x)\n",
    "#         print('5',x.size())\n",
    "        x = x.view(x.size()[0],-1)\n",
    "#         print('fc', x.size())\n",
    "        x = self.fc1(x)\n",
    "#         print('8',x.size())\n",
    "        h = self.fc2(x)\n",
    "#         print('9',h.size())\n",
    "        \n",
    "        l = self.loss(h, target)\n",
    "        return l, h, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainX, trainY, batch, device, optimizer, train_loss, train_acc):\n",
    "    model.train()\n",
    "    dloss = 0\n",
    "    dacc = 0\n",
    "    \n",
    "    rand = torch.randperm(trainX.size()[0])\n",
    "    trainX = trainX[rand]\n",
    "    trainY = trainY[rand]\n",
    "    \n",
    "    for i in range(batch[0]):\n",
    "        optimizer.zero_grad()\n",
    "        loss, output, target = model(trainX[i*batch[1]:(i+1)*batch[1]], trainY[i*batch[1]:(i+1)*batch[1]])\n",
    "        loss = loss.sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, output = torch.topk(output, 2)\n",
    "        output, _ = output.sort()\n",
    "        _, target = torch.topk(target, 2)\n",
    "        target, _ = target.sort()\n",
    "        \n",
    "        dloss += loss.cpu().item()\n",
    "        dacc += torch.sum(torch.sum(output==target, 1)==2).item()\n",
    "        \n",
    "    train_loss.append(dloss/batch[0])\n",
    "    train_acc.append(dacc/(batch[0]*batch[1]))\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test(model, testX, testY, device, test_loss, test_acc):\n",
    "    model.eval()\n",
    "    loss, output, target = model(testX, testY)\n",
    "    loss = loss.sum()\n",
    "    \n",
    "    _, output = torch.topk(output, 2)\n",
    "    output, _ = output.sort()\n",
    "    _, target = torch.topk(target, 2)\n",
    "    target, _ = target.sort()\n",
    "    \n",
    "    test_loss.append(loss.cpu().item())\n",
    "    test_acc.append(torch.sum(torch.sum(output==target, 1)==2).item()/len(testX))\n",
    "    return test_loss, test_acc, output, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]mem_0.5_model2\n",
      "mem data shape  -  0.5 ms\n",
      "train set : torch.Size([760000, 6, 3]) torch.Size([760000, 20])\n",
      "test set : torch.Size([76000, 6, 3]) torch.Size([76000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/khj/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - train loss : 0.6182222  /  test loss : 0.5493169\n",
      "           train acc : 0.0937592  /  test acc : 0.2098421\n",
      "epoch 1 - train loss : 0.5062840  /  test loss : 0.4745440\n",
      "           train acc : 0.2999934  /  test acc : 0.3644605\n",
      "epoch 2 - train loss : 0.4567018  /  test loss : 0.4387152\n",
      "           train acc : 0.4035092  /  test acc : 0.4362105\n",
      "epoch 3 - train loss : 0.4337799  /  test loss : 0.4246204\n",
      "           train acc : 0.4498947  /  test acc : 0.4645658\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-deedf804bffb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a630266f5841>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainX, trainY, batch, device, optimizer, train_loss, train_acc)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/khj/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    print('[Training]%s_%.1f_model%d'%(data_type, time_len, model_num))\n",
    "    torch.manual_seed(37)\n",
    "    torch.cuda.manual_seed_all(37)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    trainX = np.load('npy_data/%s/%s_%.1fms_trainX.npy'%(data_type, data_type, time_len))\n",
    "    trainY = np.load('npy_data/%s/%s_%.1fms_trainY.npy'%(data_type, data_type, time_len))\n",
    "    testX = np.load('npy_data/%s/%s_%.1fms_testX.npy'%(data_type, data_type, time_len))\n",
    "    testY = np.load('npy_data/%s/%s_%.1fms_testY.npy'%(data_type, data_type, time_len))\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    batch_num = 1000\n",
    "    batch = [int(len(trainX)/batch_num), batch_num]\n",
    "    \n",
    "    trainX = torch.Tensor(trainX).to(device)\n",
    "    trainY = torch.Tensor(trainY).to(device)\n",
    "    testX = torch.Tensor(testX).to(device)\n",
    "    testY = torch.Tensor(testY).to(device)\n",
    "    print('mem data shape  -  %.1f ms'%time_len)\n",
    "    print('train set :', np.shape(trainX) , np.shape(trainY))\n",
    "    print('test set :', np.shape(testX) ,np.shape(testY))\n",
    "    \n",
    "    \n",
    "    learning_rate = 0.0005\n",
    "    loss_func=nn.BCELoss()\n",
    "    model = nn.DataParallel(Model(loss_func)).to(device)\n",
    "#     model = Model(loss_func).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=batch_num, eta_min = 3e-6)\n",
    "\n",
    "#     model.load_state_dict(torch.load('ckpt/model%d/%s/%.1f_ckpt_1000.pt'%(model_num, data_type, time_len)))\n",
    "    a = time.time()\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    for epoch in range(1001):\n",
    "        train_loss, train_acc = train(model, trainX, trainY, batch, device, optimizer, train_loss, train_acc)\n",
    "        test_loss, test_acc, output, target = test(model, testX, testY, device, test_loss, test_acc)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch%1==0:\n",
    "            print('epoch %d - train loss : %.7f  /  test loss : %.7f'%(epoch, train_loss[-1], test_loss[-1]))\n",
    "            print('           train acc : %.7f  /  test acc : %.7f'%(train_acc[-1], test_acc[-1]))\n",
    "#         if epoch%50==0:\n",
    "#             print('@@@@@@@ save model : epoch %d'% epoch)\n",
    "#             torch.save(model.state_dict(),'ckpt/model%d_%s/%.1f_ckpt_%d.pt'%(model_num, data_type, time_len, epoch))\n",
    "#             np.savetxt('result/model%d_%s/%.1f_loss_tr.txt'%(model_num, data_type, time_len), train_loss)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_loss_te.txt'%(model_num, data_type, time_len), test_loss)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_acc_tr.txt'%(model_num, data_type, time_len), train_acc)\n",
    "#             np.savetxt('result/model%d_%s/%.1f_acc_te.txt'%(model_num, data_type, time_len), test_acc)\n",
    "    print(\"training complete! - calculation time :\", time.time()-a, '  seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
